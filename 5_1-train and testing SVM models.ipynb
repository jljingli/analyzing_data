{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will practice to train and test SVM models.\n",
    "\n",
    "1) Use the new dataset SST - check the accompnying README for more details\n",
    "\n",
    "2) Provide a description of the training data using descriptive statistics: how many messages are in the data? how many values for the labels? what is the distributions of the messages in the classes? what is the average number of tokens per message?\n",
    "\n",
    "3) Using FeatureUnion() and Pipeline(), develop 3 different classifier using as target classes the values for the coarse-grained labels. You can combine different features (word and character ngrams) and using different vectorisation methods (Tfidf or Count). Test your Linear SVMs against the validation/development data of SST. Select the one with best scores and apply it to the SST test data\n",
    "\n",
    "4) Apply your model to the DH_CollectingData2022_review.tsv. What is your performance? \n",
    "\n",
    "5) Save your best model using pickle. \n",
    "\n",
    "\n",
    "NOTE: if you have problems in running the model on your machine, use Google Colab.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pickle\n",
    "import csv\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_fine_grained</th>\n",
       "      <th>label_coarse_grained</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>a stirring , funny and finally transporting re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>apparently reassembled from the cutting-room f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>they presume their audience wo n't sit still f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>the entire movie is filled with deja vu moments .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>this is a visually stunning rumination on love...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label_fine_grained  label_coarse_grained  \\\n",
       "0                   4                     1   \n",
       "1                   1                    -1   \n",
       "2                   1                    -1   \n",
       "3                   2                     0   \n",
       "4                   3                     1   \n",
       "\n",
       "                                            sentence  \n",
       "0  a stirring , funny and finally transporting re...  \n",
       "1  apparently reassembled from the cutting-room f...  \n",
       "2  they presume their audience wo n't sit still f...  \n",
       "3  the entire movie is filled with deja vu moments .  \n",
       "4  this is a visually stunning rumination on love...  "
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('SST/stsa.fine.train.converted.csv', sep=',')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5860"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of messages in the train dataset\n",
    "num_messages = len(train_data)\n",
    "num_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of unique fine-grained labels in the train dataset\n",
    "num_fine_grained = train_data['label_fine_grained'].nunique()\n",
    "num_fine_grained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    1607\n",
       "1    1515\n",
       "2    1117\n",
       "4     860\n",
       "0     761\n",
       "Name: label_fine_grained, dtype: int64"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of messages in each fine-grained label class in the train dataset\n",
    "class_counts_fine_grained = train_data['label_fine_grained'].value_counts()\n",
    "class_counts_fine_grained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of unique coarse-grained labels in the train dataset\n",
    "num_coarse_grained = train_data['label_coarse_grained'].nunique()\n",
    "num_coarse_grained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    2467\n",
       "-1    2276\n",
       " 0    1117\n",
       "Name: label_coarse_grained, dtype: int64"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of messages in each coarse-grained label class in the train dataset\n",
    "class_counts_coarse_grained = train_data['label_coarse_grained'].value_counts()\n",
    "class_counts_coarse_grained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.167235494880547"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the sentences in the train dataset and count the number of tokens in each message\n",
    "train_data['tokens'] = train_data['sentence'].apply(nltk.word_tokenize)\n",
    "train_data['num_tokens'] = train_data['tokens'].apply(len)\n",
    "\n",
    "# Calculate the average number of tokens per message in the train dataset\n",
    "avg_tokens = train_data['num_tokens'].mean()\n",
    "avg_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_fine_grained</th>\n",
       "      <th>label_coarse_grained</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>in his first stab at the form , jacquot takes ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>one long string of cliches .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>if you 've ever entertained the notion of doin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>k-19 exploits our substantial collective fear ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>it 's played in the most straight-faced fashio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label_fine_grained  label_coarse_grained  \\\n",
       "0                   2                     0   \n",
       "1                   1                    -1   \n",
       "2                   1                    -1   \n",
       "3                   0                    -1   \n",
       "4                   1                    -1   \n",
       "\n",
       "                                            sentence  \n",
       "0  in his first stab at the form , jacquot takes ...  \n",
       "1                       one long string of cliches .  \n",
       "2  if you 've ever entertained the notion of doin...  \n",
       "3  k-19 exploits our substantial collective fear ...  \n",
       "4  it 's played in the most straight-faced fashio...  "
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data = pd.read_csv('SST/stsa.fine.dev.converted.csv', sep=',')\n",
    "dev_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sentences and labels from development data\n",
    "\n",
    "devSentences = dev_data['sentence']\n",
    "devLabels = dev_data['label_coarse_grained']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_fine_grained</th>\n",
       "      <th>label_coarse_grained</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>no movement , no yuks , not much of anything .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>a gob of drivel so sickly sweet , even the eag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>` how many more voyages can this limping but d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>so relentlessly wholesome it made me want to s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>gangs of new york is an unapologetic mess , wh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label_fine_grained  label_coarse_grained  \\\n",
       "0                   1                    -1   \n",
       "1                   0                    -1   \n",
       "2                   2                     0   \n",
       "3                   2                     0   \n",
       "4                   0                    -1   \n",
       "\n",
       "                                            sentence  \n",
       "0     no movement , no yuks , not much of anything .  \n",
       "1  a gob of drivel so sickly sweet , even the eag...  \n",
       "2  ` how many more voyages can this limping but d...  \n",
       "3  so relentlessly wholesome it made me want to s...  \n",
       "4  gangs of new york is an unapologetic mess , wh...  "
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv('SST/stsa.fine.test.converted.csv')\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sentences and labels from test data\n",
    "\n",
    "testSentences = test_data['sentence']\n",
    "testLabels = test_data['label_coarse_grained']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sentences and labels from training data\n",
    "trainSentences = train_data['sentence'] \n",
    "trainLabels = train_data['label_coarse_grained']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature union that combines CountVectorizer for word and character n-grams\n",
    "vectorizer_union_01 = FeatureUnion([('cnt_word', CountVectorizer(stop_words='english')),  # CountVectorizer for word n-grams\n",
    "                                    ('cnt_char', CountVectorizer(analyzer='char', ngram_range=(1, 2)))  # # CountVectorizer for character n-grams\n",
    "                               ])\n",
    "\n",
    "# Define pipeline that sequentially applies feature union and the SVM classifier with Linear Kernel\n",
    "svm_pipeline_01 = Pipeline([\n",
    "            ('vectorize', vectorizer_union_01),  # FeatureUnion transformer\n",
    "            ('classify', SVC(random_state=50, kernel='linear')) # SVM classifier with Linear Kernel\n",
    "            ])  \n",
    "\n",
    "# Fit the pipeline on the training data to learn the mapping between features and labels\n",
    "svm_pipeline_01.fit(trainSentences, trainLabels)\n",
    "\n",
    "# Predict labels for the development set using the trained pipeline\n",
    "prediction_01 = svm_pipeline_01.predict(devSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and train SVM model using feature union and pipeline for Model 1\n",
    "trained_model_feature_union1 = svm_pipeline_01.fit(trainSentences, trainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict labels for development set using Model 1\n",
    "dev_prediction1 = trained_model_feature_union1.predict(devSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION ON DEVELOPMENT\n",
      "Model 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.57      0.62      0.59       428\n",
      "           0       0.26      0.21      0.23       229\n",
      "           1       0.63      0.64      0.63       444\n",
      "\n",
      "    accuracy                           0.54      1101\n",
      "   macro avg       0.49      0.49      0.49      1101\n",
      "weighted avg       0.53      0.54      0.53      1101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print classification report for Model 1 on development set\n",
    "\n",
    "print(\"PREDICTION ON DEVELOPMENT\")\n",
    "print(\"Model 1:\")\n",
    "print(classification_report(devLabels, dev_prediction1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict labels for test set using Model 1\n",
    "test_prediction1 = trained_model_feature_union1.predict(testSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION ON TEST\n",
      "Model 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.61      0.62      0.61       912\n",
      "           0       0.20      0.19      0.19       389\n",
      "           1       0.67      0.68      0.67       909\n",
      "\n",
      "    accuracy                           0.57      2210\n",
      "   macro avg       0.49      0.49      0.49      2210\n",
      "weighted avg       0.56      0.57      0.56      2210\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print classification report for Model 1 on test set\n",
    "\n",
    "print(\"PREDICTION ON TEST\")\n",
    "print(\"Model 1:\")\n",
    "print(classification_report(testLabels, test_prediction1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 scores:\n",
      "Accuracy: 0.5660633484162896\n",
      "Macro F1: 0.49254851081937323\n",
      "Weighted F1: 0.5638487037126336\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy, macro F1, and weighted F1 scores for Model 1 on test set\n",
    "\n",
    "accuracy_1 = accuracy_score(testLabels, test_prediction1)\n",
    "macro_f1_1 = f1_score(testLabels, test_prediction1, average='macro')\n",
    "weighted_f1_1 = f1_score(testLabels, test_prediction1, average='weighted')\n",
    "\n",
    "# Print Model 1 scores\n",
    "\n",
    "print(\"Model 1 scores:\")\n",
    "print(\"Accuracy:\", accuracy_1)\n",
    "print(\"Macro F1:\", macro_f1_1)\n",
    "print(\"Weighted F1:\", weighted_f1_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature union and pipeline\n",
    "vectorizer_union_02 = FeatureUnion([('tfidf_word', TfidfVectorizer(stop_words='english')),\n",
    "                                    ('tfidf_char', TfidfVectorizer(analyzer='char', ngram_range=(1, 2)))])\n",
    "svm_pipeline_02 = Pipeline([\n",
    "            ('vectorize', vectorizer_union_02),\n",
    "            ('classify', SVC(random_state=50, kernel='linear'))\n",
    "            ])\n",
    "\n",
    "# Train the model and make predictions on development set\n",
    "trained_model_feature_union2 = svm_pipeline_02.fit(trainSentences, trainLabels)\n",
    "dev_prediction2 = trained_model_feature_union2.predict(devSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION ON DEVELOPMENT\n",
      "Model 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.61      0.69      0.65       428\n",
      "           0       0.31      0.13      0.18       229\n",
      "           1       0.63      0.74      0.68       444\n",
      "\n",
      "    accuracy                           0.59      1101\n",
      "   macro avg       0.52      0.52      0.50      1101\n",
      "weighted avg       0.55      0.59      0.56      1101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print classification report for development set\n",
    "\n",
    "print(\"PREDICTION ON DEVELOPMENT\")\n",
    "print(\"Model 2:\")\n",
    "print(classification_report(devLabels, dev_prediction2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION ON TEST\n",
      "Model 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.65      0.72      0.69       912\n",
      "           0       0.26      0.10      0.15       389\n",
      "           1       0.67      0.78      0.72       909\n",
      "\n",
      "    accuracy                           0.64      2210\n",
      "   macro avg       0.53      0.53      0.52      2210\n",
      "weighted avg       0.59      0.64      0.61      2210\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on test set and print classification report and scores\n",
    "\n",
    "test_prediction2 = trained_model_feature_union2.predict(testSentences)\n",
    "\n",
    "print(\"PREDICTION ON TEST\")\n",
    "print(\"Model 2:\")\n",
    "print(classification_report(testLabels, test_prediction2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2 scores:\n",
      "Accuracy: 0.6357466063348416\n",
      "Macro F1: 0.518419290691118\n",
      "Weighted F1: 0.6059621637227968\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy, macro F1, and weighted F1 scores for Model 2\n",
    "\n",
    "accuracy_2 = accuracy_score(testLabels, test_prediction2)\n",
    "macro_f1_2 = f1_score(testLabels, test_prediction2, average='macro')\n",
    "weighted_f1_2 = f1_score(testLabels, test_prediction2, average='weighted')\n",
    "\n",
    "print(\"Model 2 scores:\")\n",
    "print(\"Accuracy:\", accuracy_2)\n",
    "print(\"Macro F1:\", macro_f1_2)\n",
    "print(\"Weighted F1:\", weighted_f1_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature union and pipeline\n",
    "vectorizer_union_03 = FeatureUnion([('cnt_word', CountVectorizer(stop_words='english')),\n",
    "                               ('tfidf_char', TfidfVectorizer(analyzer='char', ngram_range=(1, 2)))\n",
    "                               ])\n",
    "\n",
    "svm_pipeline_03 = Pipeline([\n",
    "            ('vectorize', vectorizer_union_03),\n",
    "            ('classify', SVC(random_state=50, kernel='linear'))\n",
    "            ])\n",
    "\n",
    "trained_model_feature_union3 = svm_pipeline_03.fit(trainSentences, trainLabels)\n",
    "dev_prediction3 = trained_model_feature_union3.predict(devSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION ON DEVELOPMENT\n",
      "Model 3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.58      0.64      0.61       428\n",
      "           0       0.25      0.17      0.20       229\n",
      "           1       0.62      0.66      0.64       444\n",
      "\n",
      "    accuracy                           0.55      1101\n",
      "   macro avg       0.48      0.49      0.48      1101\n",
      "weighted avg       0.53      0.55      0.54      1101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"PREDICTION ON DEVELOPMENT\")\n",
    "print(\"Model 3:\")\n",
    "print(classification_report(devLabels, dev_prediction3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set and print classification report and scores\n",
    "\n",
    "test_prediction3 = trained_model_feature_union3.predict(testSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION ON TEST\n",
      "Model 3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.62      0.66      0.64       912\n",
      "           0       0.22      0.19      0.20       389\n",
      "           1       0.68      0.68      0.68       909\n",
      "\n",
      "    accuracy                           0.58      2210\n",
      "   macro avg       0.51      0.51      0.51      2210\n",
      "weighted avg       0.58      0.58      0.58      2210\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"PREDICTION ON TEST\")\n",
    "print(\"Model 3:\")\n",
    "print(classification_report(testLabels, test_prediction3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 3 scores:\n",
      "Accuracy: 0.5846153846153846\n",
      "Macro F1: 0.5073185463780766\n",
      "Weighted F1: 0.5796806609875855\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy, macro F1, and weighted F1 scores for Model 3\n",
    "accuracy_3 = accuracy_score(testLabels, test_prediction3)\n",
    "macro_f1_3 = f1_score(testLabels, test_prediction3, average='macro')\n",
    "weighted_f1_3 = f1_score(testLabels, test_prediction3, average='weighted')\n",
    "\n",
    "print(\"Model 3 scores:\")\n",
    "print(\"Accuracy:\", accuracy_3)\n",
    "print(\"Macro F1:\", macro_f1_3)\n",
    "print(\"Weighted F1:\", weighted_f1_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the evaluation metrics, Model 2 has the highest accuracy, macro F1, and weighted F1 scores. Therefore, Model 2 is the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>For Nik, he only wants to silence the cacophon...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"I can play this two ways</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mild, because it isn't conclusive, and doesn't...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You can also get some more information about t...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Soon, Hero, who has never had friends, is thru...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0    1\n",
       "0  For Nik, he only wants to silence the cacophon...  0.0\n",
       "1                          \"I can play this two ways  0.0\n",
       "2  Mild, because it isn't conclusive, and doesn't... -1.0\n",
       "3  You can also get some more information about t...  0.0\n",
       "4  Soon, Hero, who has never had friends, is thru...  0.0"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "dh_data = pd.read_csv('DH_CollectingData2022_review.tsv', sep = '\\t', header = None, quoting = csv.QUOTE_NONE)\n",
    "dh_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>For Nik, he only wants to silence the cacophon...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"I can play this two ways</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mild, because it isn't conclusive, and doesn't...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You can also get some more information about t...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Soon, Hero, who has never had friends, is thru...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  sentiment\n",
       "0  For Nik, he only wants to silence the cacophon...        0.0\n",
       "1                          \"I can play this two ways        0.0\n",
       "2  Mild, because it isn't conclusive, and doesn't...       -1.0\n",
       "3  You can also get some more information about t...        0.0\n",
       "4  Soon, Hero, who has never had friends, is thru...        0.0"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dh_data = dh_data.rename(columns={0: 'sentence', 1: 'sentiment'})\n",
    "dh_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.23      0.74      0.36        62\n",
      "         0.0       0.50      0.10      0.17       146\n",
      "         1.0       0.61      0.55      0.58       180\n",
      "\n",
      "    accuracy                           0.41       388\n",
      "   macro avg       0.45      0.46      0.37       388\n",
      "weighted avg       0.51      0.41      0.39       388\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the trained model to predict sentiment for the dhSentences\n",
    "# Drop any missing values in dh_data\n",
    "\n",
    "dh_data = dh_data.dropna()\n",
    "\n",
    "# Extract the sentences and labels from the data\n",
    "dhSentences = dh_data['sentence']\n",
    "dhLabels = dh_data['sentiment']\n",
    "\n",
    "# Use the trained model to predict sentiment for the dhSentences\n",
    "dh_predictions = trained_model_feature_union2.predict(dhSentences)\n",
    "\n",
    "# Print the classification report for the dh data\n",
    "print(classification_report(dhLabels, dh_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the classification report, the performance of the model is not very good. The accuracy is 0.41, which means that only 41% of the predictions were correct. The macro average F1-score is 0.37, which indicates that the model is not performing well in predicting any of the classes. The weighted average F1-score is 0.39, which is slightly better but still indicates poor performance. The precision and recall values for each class are also quite low, indicating that the model is not able to correctly classify the comments into the three sentiment categories (-1, 0, 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model to a file\n",
    "with open('best_model.pkl', 'wb') as file:\n",
    "    pickle.dump(trained_model_feature_union3, file)\n",
    "    \n",
    "# Load the saved model from a file\n",
    "with open('best_model.pkl', 'rb') as file:\n",
    "    loaded_model = pickle.load(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
