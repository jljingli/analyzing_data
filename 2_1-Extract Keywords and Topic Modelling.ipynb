{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf619604",
   "metadata": {},
   "source": [
    "# Keyword Extraction and Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8927d8b",
   "metadata": {},
   "source": [
    "At the very beginning, the most basic step is to install and import some necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d05a5d1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Users/jo/opt/anaconda3/lib/python3.9/site-packages (4.3.0)\n",
      "Requirement already satisfied: FuzzyTM>=0.4.0 in /Users/jo/opt/anaconda3/lib/python3.9/site-packages (from gensim) (2.0.5)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/jo/opt/anaconda3/lib/python3.9/site-packages (from gensim) (5.1.0)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Users/jo/opt/anaconda3/lib/python3.9/site-packages (from gensim) (1.10.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/jo/opt/anaconda3/lib/python3.9/site-packages (from gensim) (1.24.2)\n",
      "Requirement already satisfied: pandas in /Users/jo/opt/anaconda3/lib/python3.9/site-packages (from FuzzyTM>=0.4.0->gensim) (1.4.2)\n",
      "Requirement already satisfied: pyfume in /Users/jo/opt/anaconda3/lib/python3.9/site-packages (from FuzzyTM>=0.4.0->gensim) (0.2.25)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/jo/opt/anaconda3/lib/python3.9/site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/jo/opt/anaconda3/lib/python3.9/site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/jo/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->FuzzyTM>=0.4.0->gensim) (1.16.0)\n",
      "Requirement already satisfied: fst-pso in /Users/jo/opt/anaconda3/lib/python3.9/site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (1.8.1)\n",
      "Requirement already satisfied: simpful in /Users/jo/opt/anaconda3/lib/python3.9/site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (2.10.0)\n",
      "Requirement already satisfied: miniful in /Users/jo/opt/anaconda3/lib/python3.9/site-packages (from fst-pso->pyfume->FuzzyTM>=0.4.0->gensim) (0.0.6)\n",
      "Requirement already satisfied: requests in /Users/jo/opt/anaconda3/lib/python3.9/site-packages (from simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/jo/opt/anaconda3/lib/python3.9/site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/jo/opt/anaconda3/lib/python3.9/site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jo/opt/anaconda3/lib/python3.9/site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jo/opt/anaconda3/lib/python3.9/site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2021.10.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3445b10f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /Users/jo/opt/anaconda3/lib/python3.9/site-packages (1.2.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/jo/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/jo/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.24.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/jo/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/jo/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.10.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d898b453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install and import some necessary libraries \n",
    "import pandas as pd  # for data manipulation and analysis\n",
    "import nltk          # for natural language processing\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "290ad453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re                   \n",
    "import numpy as np          # for numerical calculations\n",
    "from pprint import pprint   # print data structures in a readable, pretty way\n",
    "import gensim               # a Python library for topic modelling\n",
    "import gensim.corpora as corpora \n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import pyLDAvis             # visualize topic modelling\n",
    "import pyLDAvis.gensim_models\n",
    "import matplotlib.pyplot as plt \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc765e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c841e5b1",
   "metadata": {},
   "source": [
    "## Upload the dataset and Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10afe2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in a tsv file with no header row\n",
    "df = pd.read_csv('DH_CollectingData2022_review.tsv', sep = '\\t', header = None, quoting = csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78ef1c38",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>For Nik, he only wants to silence the cacophon...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"I can play this two ways</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mild, because it isn't conclusive, and doesn't...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You can also get some more information about t...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Soon, Hero, who has never had friends, is thru...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0    1\n",
       "0  For Nik, he only wants to silence the cacophon...  0.0\n",
       "1                          \"I can play this two ways  0.0\n",
       "2  Mild, because it isn't conclusive, and doesn't... -1.0\n",
       "3  You can also get some more information about t...  0.0\n",
       "4  Soon, Hero, who has never had friends, is thru...  0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdc6515",
   "metadata": {},
   "source": [
    "Since the columns don't have names, I renamed them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1f3459d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the columns \n",
    "df = df.rename(columns={0: 'texts', 1: 'sentiment'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1109c85d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>For Nik, he only wants to silence the cacophon...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"I can play this two ways</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mild, because it isn't conclusive, and doesn't...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You can also get some more information about t...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Soon, Hero, who has never had friends, is thru...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               texts  sentiment\n",
       "0  For Nik, he only wants to silence the cacophon...        0.0\n",
       "1                          \"I can play this two ways        0.0\n",
       "2  Mild, because it isn't conclusive, and doesn't...       -1.0\n",
       "3  You can also get some more information about t...        0.0\n",
       "4  Soon, Hero, who has never had friends, is thru...        0.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b0332e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "texts         object\n",
       "sentiment    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfab755",
   "metadata": {},
   "source": [
    "In this case, I change the data type in the sentiment column from float number to integer number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59bef0f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "texts        0\n",
       "sentiment    2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the missing values\n",
    "\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec88d126",
   "metadata": {},
   "source": [
    "There are two missing values in the 'sentiment' column, and in this case, I deleted all the missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "379c0b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the rows with missing values\n",
    "df.dropna(inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9d2a898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the data type\n",
    "df['sentiment'] = df['sentiment'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30f459a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>For Nik, he only wants to silence the cacophon...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"I can play this two ways</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mild, because it isn't conclusive, and doesn't...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You can also get some more information about t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Soon, Hero, who has never had friends, is thru...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               texts  sentiment\n",
       "0  For Nik, he only wants to silence the cacophon...          0\n",
       "1                          \"I can play this two ways          0\n",
       "2  Mild, because it isn't conclusive, and doesn't...         -1\n",
       "3  You can also get some more information about t...          0\n",
       "4  Soon, Hero, who has never had friends, is thru...          0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "967cb95a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(388, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check how many rows and columns in the DataFrame\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9452ae",
   "metadata": {},
   "source": [
    "## Extract Top 20 keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0af1343",
   "metadata": {},
   "source": [
    "By modifying the code for tf-idf, extract the top 20 keywords for the whole dataset, firstly, I choose \"texts\" to create a data series that contains all the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84f81043",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      For Nik, he only wants to silence the cacophon...\n",
      "1                              \"I can play this two ways\n",
      "2      Mild, because it isn't conclusive, and doesn't...\n",
      "3      You can also get some more information about t...\n",
      "4      Soon, Hero, who has never had friends, is thru...\n",
      "                             ...                        \n",
      "385    August is torn by his actions but he absolutel...\n",
      "386    Heroine Elise Benton is witty in the present d...\n",
      "387                 I am glad there will be a part three\n",
      "388    Sometimes while they were in a lesson, the wol...\n",
      "389                                  I remained involved\n",
      "Name: texts, Length: 388, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_texts = df['texts']\n",
    "print(df_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "205cdf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code retrived from: https://www.analyticsvidhya.com/blog/2021/06/must-known-techniques-for-text-preprocessing-in-nlp/#:~:text=Text%20preprocessing%20is%20a%20method,text%20in%20a%20different%20case.\n",
    "\n",
    "# use to stem the words into root word\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_words(text):\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "df['df_texts'] = df['texts'].apply(lambda text: lemmatize_words(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05f3c24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation\n",
    "import string\n",
    "\n",
    "df['df_texts'] = df['texts'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '' , x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d7c32fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase \n",
    "df['df_texts'] = df['df_texts'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1fca075",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      for nik he only wants to silence the cacophony...\n",
       "1                               i can play this two ways\n",
       "2      mild because it isnt conclusive and doesnt giv...\n",
       "3      you can also get some more information about t...\n",
       "4      soon hero who has never had friends is thrust ...\n",
       "                             ...                        \n",
       "385    august is torn by his actions but he absolutel...\n",
       "386    heroine elise benton is witty in the present d...\n",
       "387                 i am glad there will be a part three\n",
       "388    sometimes while they were in a lesson the wolf...\n",
       "389                                  i remained involved\n",
       "Name: df_texts, Length: 388, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['df_texts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0fa5fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all the sentences in the list into a single text\n",
    "all_texts = ' '.join(df['df_texts'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c242ce",
   "metadata": {},
   "source": [
    "This will be used to extract the top 20 keywords for the whole dataset using tf-idf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb31eaa1",
   "metadata": {},
   "source": [
    "### TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82724d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove common English words that won't be of any value for keyword extraction\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# apply TfidfVectorizer to the all texts and transform the texts into a matrix of TF-IDF values\n",
    "texts_tfidf = tfidf_vectorizer.fit_transform([all_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1434d27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        100        12        14        15     1920s        20      2013  \\\n",
      "0  0.006966  0.006966  0.013931  0.006966  0.006966  0.006966  0.006966   \n",
      "\n",
      "         46        50       600  ...      year     years    yelled   yelling  \\\n",
      "0  0.006966  0.020897  0.006966  ...  0.020897  0.013931  0.013931  0.006966   \n",
      "\n",
      "      youll     young     youre     youve      zero     zusak  \n",
      "0  0.006966  0.006966  0.013931  0.006966  0.006966  0.006966  \n",
      "\n",
      "[1 rows x 1571 columns]\n"
     ]
    }
   ],
   "source": [
    "# Stores the TF-IDF values into a DataFrame, with each row representing a text and each column representing a word\n",
    "texts_tfidf_matrix = pd.DataFrame(texts_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "print(texts_tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4c95f0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [('book', 0.5224229744107162), ('read', 0.3134537846464297), ('story', 0.2437973880583342), ('just', 0.18110663112904826), ('characters', 0.16717535181142917), ('good', 0.13931279317619097), ('im', 0.13931279317619097), ('like', 0.1323471535173814), ('really', 0.1323471535173814), ('great', 0.11145023454095278), ('love', 0.11145023454095278), ('novel', 0.11145023454095278), ('reading', 0.09751895522333368), ('think', 0.09751895522333368), ('character', 0.09055331556452413), ('did', 0.08358767590571459), ('books', 0.07662203624690503), ('doesnt', 0.07662203624690503), ('interesting', 0.07662203624690503), ('liked', 0.07662203624690503)]}\n"
     ]
    }
   ],
   "source": [
    "# convert the concatenated dataframe into a dictionary\n",
    "words_dict_tfidf = texts_tfidf_matrix.to_dict('index')\n",
    "\n",
    "# store the top 20 keywords for each row and print them\n",
    "word_repr_tfidf = {}\n",
    "for texts_id, target_words in words_dict_tfidf.items():\n",
    "    list_targets = [(k, v) for k, v in target_words.items()]\n",
    "    list_targets_sorted = sorted(list_targets, key=lambda x: x[1], reverse=True)\n",
    "    word_repr_tfidf[texts_id] = list_targets_sorted[0:20]\n",
    "\n",
    "print(word_repr_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d36923",
   "metadata": {},
   "source": [
    "The output shows that the top 20 keywords for the whole dataset are: 'book', 'read', 'story', 'just', 'characters', 'good', 'im', 'like', 'really', 'great', 'love', 'novel', 'reading', 'think', 'character', 'did', 'books', 'doesnt', 'interesting', ‘liked'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b499a59b",
   "metadata": {},
   "source": [
    "# Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba1d891",
   "metadata": {},
   "source": [
    "### Topic Model for the Positive Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d8e72c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5     i did like steven (or stephen? i listened to t...\n",
       "6     the plot is quick moving and the action is vio...\n",
       "7                      loved everything about this book\n",
       "9                                     great, quick read\n",
       "10    although there isn't character development, as...\n",
       "Name: texts, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a new dataframe with only positive sentiment\n",
    "texts_pos = df.loc[df['sentiment'] == 1]['texts']\n",
    "texts_pos = texts_pos.apply(lambda x: x.lower())\n",
    "texts_pos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196b390f",
   "metadata": {},
   "source": [
    "#### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e8483a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove non-alphabetic characters\n",
    "\n",
    "def remove_non_alphabetic(texts):\n",
    "    return [re.sub('[^a-zA-Z]', ' ', str(doc)) for doc in texts]\n",
    "    \n",
    "texts_pos_clean = remove_non_alphabetic(texts_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "30fa12d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words from each positive text\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "texts_pos_nostops = remove_stopwords(texts_pos_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "96e34d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce inflected words to their root word\n",
    "\n",
    "def lemmatize(texts):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [[lemmatizer.lemmatize(word) for word in doc] for doc in texts]\n",
    "\n",
    "texts_pos_lemmatized = lemmatize(texts_pos_nostops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1002984",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_pos_ppr = texts_pos_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f1a1d829",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['like', 'steven', 'stephen', 'listened', 'book'], ['plot', 'quick', 'moving', 'action', 'violent'], ['loved', 'everything', 'book'], ['great', 'quick', 'read'], ['although', 'character', 'development', 'case', 'mystery', 'novel', 'yet', 'reader', 'come', 'enlightened', 'several', 'notion', 'idea', 'life'], ['liked', 'ending'], ['believe', 'barnes', 'delivers', 'promise', 'book', 'garden', 'stone', 'well', 'worth', 'read'], ['finish', 'pig', 'island', 'really', 'like', 'ending', 'read', 'hanging', 'hill'], ['narrator', 'good', 'audiobook', 'seven', 'hour'], ['great', 'story', 'girl', 'friendship', 'dog'], ['read', 'book', 'admire'], ['year', 'old', 'loved'], ['much', 'liked', 'main', 'character', 'never', 'questioned', 'acted', 'reacted', 'bizarre', 'situation', 'impressive'], ['think', 'another', 'reason', 'enjoyed', 'much', 'reminded', 'bit'], ['read', 'hayder', 'hanging', 'hill', 'enjoyed', 'ending', 'maybe', 'yelled', 'book', 'little', 'bit', 'read', 'pig', 'island'], ['cathy', 'ace', 'expertly', 'weave', 'comic', 'relief', 'also', 'first', 'page', 'relieve', 'tension', 'built', 'first', 'page', 'novel'], ['good', 'first', 'novel', 'planning', 'reading', 'follow', 'story', 'pretty', 'solid', 'author', 'kept', 'intriguing', 'enough', 'general', 'enjoy', 'adventure'], ['love', 'much'], ['also', 'first', 'novel', 'read', 'fully', 'ipad', 'saying', 'something', 'much', 'prefer', 'feel', 'book', 'hand', 'novel', 'ipad', 'remained', 'unread'], ['barnes', 'present', 'garden', 'stone', 'well', 'developed', 'fully', 'laid', 'world'], ['spell', 'binding', 'pageturner'], ['clever', 'story', 'witty', 'likeable', 'main', 'character'], ['series', 'twist', 'turn', 'august', 'creates', 'situation', 'nicole', 'become', 'blood', 'mate', 'give', 'easily', 'strong', 'feisty', 'heroine'], ['fit', 'pretty', 'carefree'], ['also', 'audiobook', 'really', 'nice', 'afterword', 'hill', 'talk', 'writing', 'book', 'audiobooks', 'give', 'audio', 'recommendation'], ['trilogy', 'book', 'getting', 'point', 'book', 'point', 'book', 'presumably', 'resolution', 'full', 'nice', 'bit', 'action', 'building', 'tension', 'third', 'book'], ['like', 'many', 'good', 'fantasy', 'abercrombie', 'try', 'hide', 'grit', 'behind', 'feat', 'magic', 'enchanted', 'sword'], ['part', 'leopard', 'part', 'rat', 'fink', 'rucpard', 'speaks', 'third', 'person', 'absolutely', 'adorable', 'despite', 'kilo', 'weight', 'defensive', 'charge', 'hero'], ['solid', 'police', 'procedural', 'good', 'start', 'series'], ['great', 'nonfiction', 'older', 'kid', 'story', 'time'], ['kavach', 'building', 'certainly', 'mystery', 'solved', 'clue', 'waiting', 'side', 'room'], ['great', 'little', 'book', 'around'], ['reading', 'alone', 'adult', 'love', 'snicket', 'story', 'would', 'fun', 'illustration', 'listening', 'terry', 'gross', 'read', 'story', 'kidnapped', 'newt'], ['admit', 'enjoyed', 'ended', 'book', 'loved', 'made', 'reader', 'think', 'hard', 'sebastian', 'truthful', 'plan'], ['love', 'dex'], ['hooked', 'wait', 'read', 'rest', 'series'], ['totally', 'see', 'coming', 'however', 'predict', 'ending', 'even', 'reading', 'book'], ['narrator', 'ray', 'porter', 'great'], ['two', 'three', 'main', 'character', 'girl', 'vampire', 'fang', 'dragon', 'wing', 'magic', 'potion', 'sight', 'refreshing'], ['felt', 'bit', 'like', 'nicely', 'written', 'history', 'book'], ['story', 'merges', 'post', 'apocalyptic', 'future', 'caribbean', 'voodoo', 'context', 'character', 'multi', 'generational', 'well', 'written'], ['strong', 'sense', 'right', 'wrong', 'even', 'mean', 'action', 'hurt'], ['soon', 'forget', 'character', 'created', 'emma', 'scott'], ['scanned', 'review', 'likening', 'lost', 'loved', 'first', 'couple', 'season', 'show', 'think', 'would', 'disservice', 'compare', 'something', 'well', 'thought'], ['rich', 'girl', 'poor', 'girl', 'russian', 'folklore', 'really', 'liked'], ['constantly', 'wondering', 'would', 'die', 'next', 'committing', 'murder'], ['horn', 'fit', 'bill', 'glad', 'read'], ['became', 'fan', 'read', 'story', 'foretold', 'excited', 'read', 'book'], ['loved', 'book'], ['mystery', 'produce', 'mystery', 'keep', 'reader', 'mind', 'working'], ['could', 'literally', 'stare', 'lip', 'day', 'never', 'get', 'bored'], ['star'], ['beautiful', 'illustration', 'good', 'mix', 'technical', 'detail', 'interesting', 'bit', 'life', 'wright', 'brother', 'two', 'middle', 'brother', 'five', 'sibling'], ['colleen', 'hoover', 'love', 'story', 'leave', 'speechless', 'emotional'], ['novel', 'fairly', 'paced'], ['awesome'], ['perfect', 'wonderful', 'marvelous', 'little', 'tooth', 'rotting', 'really'], ['book', 'also', 'fascinating', 'glimpse', 'lonely', 'life', 'writer', 'almost', 'obsessive', 'need', 'get', 'word', 'cost'], ['cool', 'story', 'loved', 'main', 'character', 'jack'], ['much', 'way', 'lindqvist', 'treat', 'character', 'way', 'navigates', 'narrative'], ['highly', 'recommend'], ['seems', 'change', 'left', 'mark', 'series', 'end', 'harry', 'dresden', 'chicago', 'wizard', 'investigator', 'start', 'new', 'series', 'harry', 'dresden', 'superpowered', 'wizard'], ['pressed', 'death', 'kirsten', 'wei', 'start', 'line', 'cozy', 'possibly', 'paranormal', 'mystery', 'novel', 'exciting', 'quick', 'paced', 'fun'], ['often', 'picky', 'sci', 'fi', 'book', 'read', 'hero', 'definitely', 'alley'], ['simply', 'stunning', 'emma', 'scott', 'writes', 'lyrical', 'gorgeous', 'fiction'], ['exciting', 'hero', 'belinda', 'crawford', 'quick', 'paced', 'point'], ['trilogy', 'certainly', 'get', 'popular', 'attention', 'deserves'], ['help', 'webster', 'read', 'little', 'like', 'raylan', 'given', 'arliss', 'howard', 'low', 'melodic', 'softly', 'twangy', 'voice', 'elmore', 'leonard', 'always', 'writes', 'fast', 'fun', 'story'], ['really', 'wanted', 'book', 'instead', 'separate', 'tale'], ['enjoyed', 'story', 'much'], ['direction', 'took', 'mature', 'content', 'theme', 'emphasis', 'psychological', 'physical', 'torture', 'happening', 'one', 'thing', 'adrian', 'sydney', 'get', 'married', 'holy', 'crap', 'literally', 'yelled', 'loud', 'read', 'part'], ['begin', 'one', 'night', 'stand', 'zero', 'expectation', 'future', 'turn', 'burning', 'hot', 'love', 'deep', 'unbreakable', 'connection'], ['really', 'liked', 'story'], ['another', 'excellent', 'story'], ['kitty', 'thomas', 'could', 'create', 'cast', 'strong', 'character', 'like', 'blood', 'lust', 'fantastically', 'creative', 'story', 'keep', 'guessing', 'end'], ['boy', 'girl', 'sure', 'enjoy'], ['never', 'million', 'year', 'think', 'would', 'love', 'book', 'parallel', 'world', 'elf', 'king', 'magic'], ['reader', 'catapulted', 'conflict', 'head', 'first', 'although', 'first', 'chapter', 'confusing', 'chaotic', 'scene', 'fighting', 'faction', 'war', 'understand', 'make', 'sense', 'soon', 'enough'], ['hoover', 'actually', 'made', 'fall', 'love', 'holder', 'already'], ['worth', 'stay', 'past', 'bedtime', 'get', 'finished'], ['excellent', 'terrifying', 'book'], ['narrated', 'first', 'person', 'perspective', 'cait', 'morgan', 'novel', 'fun', 'mind', 'provoking', 'cozy', 'mystery'], ['whole', 'fast', 'read', 'good', 'slice', 'brain', 'candy', 'thriller'], ['must', 'gone', 'box', 'tissue', 'reading'], ['hill', 'take', 'exploration', 'devil', 'evil', 'original', 'interesting', 'okay', 'minor', 'issue'], ['twisting', 'kingdom', 'together', 'great', 'adventure', 'due', 'released', 'december', 'definite', 'must', 'lover', 'medieval', 'also', 'good', 'page', 'turner'], ['fun', 'exciting', 'adventure', 'featuring', 'favourite', 'prince', 'charming', 'princess'], ['like', 'lot', 'high', 'school', 'drama', 'book', 'broken', 'prince', 'reminded', 'gossip', 'girl', 'many', 'way'], ['cute', 'take', 'jane', 'austen', 'pride', 'prejudice', 'set', 'amid', 'privileged', 'offspring', 'hollywood', 'royalty'], ['cute', 'sophie', 'want', 'eat', 'squash', 'bought', 'market', 'keep', 'friend', 'companion', 'well', 'little', 'squash', 'start', 'getting', 'brown', 'spot', 'sweet', 'fun'], ['love', 'way', 'make', 'feel', 'truly', 'south'], ['must', 'check'], ['lovely', 'book', 'constant', 'search', 'new', 'reading', 'material', 'child', 'sparked', 'curiosity'], ['particularly', 'kind', 'enjoyed', 'fact', 'devil', 'superhero', 'right', 'light'], ['name', 'specie', 'faction', 'keep', 'straight', 'barnes', 'world', 'fully', 'realized', 'departure', 'familiar'], ['girl', 'exuded', 'calm', 'danced', 'along', 'skin', 'inhaled', 'like', 'breathing', 'scent', 'something', 'delicious', 'far', 'away'], ['disappointed'], ['read', 'first', 'five', 'page', 'thought', 'lois', 'duncan', 'really', 'great', 'writer', 'got', 'five', 'page', 'realized', 'duncan', 'great', 'writer', 'tell', 'one', 'hell', 'story'], ['happy', 'tell'], ['slow', 'start', 'beginning', 'although', 'exposition', 'full', 'little', 'awkward', 'slow', 'build', 'sanderson', 'provides', 'stunningly', 'well', 'thought', 'magic', 'system', 'something', 'known', 'good', 'reason', 'well', 'high', 'paced', 'well', 'written', 'fantasy', 'action'], ['anything', 'book', 'quick', 'enjoyable', 'read', 'keep', 'edge', 'seat'], ['really', 'enjoyed'], ['fairy', 'tale', 'enough', 'kid', 'bloody', 'enough', 'u', 'fan'], ['amazing', 'higginson', 'one', 'admired', 'author'], ['reader', 'perspective', 'watching', 'jorg', 'take', 'bite', 'painful', 'bite', 'heart', 'bitter'], ['gorgeous', 'star'], ['even', 'though', 'rachel', 'completely', 'unlikable', 'cheering', 'throughout', 'book'], ['think', 'hill', 'played', 'devilish', 'reference', 'bit', 'much', 'evil', 'knieval', 'trail', 'number', 'time', 'throw', 'book', 'still', 'fun', 'read', 'well', 'worth', 'price', 'admission'], ['alchemist', 'shadow', 'good', 'read', 'may'], ['actually', 'perfect', 'great', 'voice', 'variety', 'character', 'accent'], ['plan', 'recommend', 'creative', 'writing', 'student', 'love', 'genre', 'much', 'experience', 'drawing', 'character', 'integrating', 'conflict', 'world', 'created'], ['suspense', 'kept', 'great'], ['started', 'could', 'put'], ['prologue', 'immediately', 'fell', 'love', 'm'], ['ingredient', 'great', 'read'], ['fun', 'fantastical', 'mystery', 'full', 'oddball', 'character', 'bearded', 'lady', 'time', 'traveling', 'ugly', 'man', 'may', 'may', 'founded', 'london', 'two', 'assassin', 'dressed', 'like', 'overgrown', 'prep', 'school', 'drop', 'out', 'human', 'fly', 'course', 'titular', 'side', 'kick', 'somnambulist'], ['think', 'could', 'make', 'good', 'read', 'aloud', 'grade'], ['markus', 'zusak', 'made', 'post', 'enjoyed', 'book'], ['thank', 'party', 'opportunity', 'read', 'book', 'advance', 'hard', 'holding', 'review', 'closer', 'publication'], ['read', 'time', 'said', 'year', 'old', 'love', 'pick', 'nose'], ['buy'], ['make', 'really', 'tough', 'begin', 'hopefully', 'everything', 'work', 'next', 'book'], ['time', 'passed', 'world', 'turned', 'say', 'gave', 'book', 'another', 'shot'], ['ghost', 'see', 'lot', 'character', 'background', 'help', 'reader', 'form', 'better', 'picture', 'character'], ['would', 'definitely', 'like', 'read', 'character', 'story'], ['book', 'excellent', 'example', 'really', 'good', 'third', 'person', 'mystery'], ['well', 'liked'], ['feel', 'good'], ['dark', 'every', 'page', 'drip', 'said', 'tension', 'knowing', 'someone', 'probably', 'everyone', 'lying', 'thick', 'thick', 'thick'], ['character', 'well', 'developed', 'especially', 'hero', 'soon', 'realises', 'much', 'special'], ['wanted', 'maggie', 'think', 'book', 'often'], ['book', 'extremely', 'well', 'create', 'tension', 'suspense'], ['although', 'really', 'liked', 'demille', 'character', 'grew', 'previous', 'novel', 'appeared', 'took', 'awhile', 'really', 'fall', 'book', 'become', 'engaged', 'story'], ['coming', 'home', 'read'], ['loved', 'book'], ['needed', 'something', 'fun', 'read', 'knew', 'take', 'seriously', 'outside', 'usual', 'reading', 'circle', 'without', 'foreign'], ['alessandra', 'torre', 'deserves', 'accolade', 'one'], ['lot', 'absolutely', 'unbelievable', 'plot', 'point', 'mind', 'much'], ['lot', 'beautiful', 'line', 'image', 'novel'], ['well', 'written', 'easy', 'read', 'debut', 'novel'], ['leaf', 'alone', 'middle', 'wood', 'begin', 'epic', 'tale', 'swooning', 'every', 'word'], ['one', 'sitting', 'tear', 'awesome'], ['ride', 'loved'], ['many', 'great', 'twist', 'turn'], ['felt', 'pain', 'pressure', 'try', 'protect', 'innocent', 'ella', 'cost'], ['certainly', 'read'], ['pure', 'americana', 'rough', 'raspy', 'great', 'hero', 'even', 'better', 'villain'], ['guess', 'could', 'interpreted', 'good', 'way', 'bring', 'reader', 'back', 'following', 'book'], ['writing', 'duo', 'make', 'erin', 'watt', 'incredibly', 'talented', 'love', 'book'], ['thought', 'going', 'go', 'direction', 'enjoy', 'discovered', 'antagonist', 'pleasantly', 'surprised'], ['best', 'book', 'bloodline', 'series', 'far', 'opinion'], ['many', 'lambasted', 'novel', 'dark', 'misogynistic', 'admit', 'see', 'read'], ['description', 'use', 'novelistic', 'element', 'tell', 'story', 'work', 'beautifully', 'bring', 'dimension', 'another', 'writer', 'would', 'gotten'], ['looking', 'forward', 'rest'], ['please', 'put', 'second', 'book', 'series'], ['tale', 'revenge', 'rich', 'single', 'mindedness', 'classic', 'duma'], ['particularly', 'kind', 'enjoyed', 'fact', 'devil', 'superhero', 'right', 'light'], ['looking', 'summer', 'pool', 'beach', 'read', 'pick', 'broken', 'prince', 'enjoy', 'crazy', 'ride'], ['sas', 'strength', 'lot', 'humanity', 'non', 'human'], ['meet', 'pure', 'magic'], ['crawford', 'done', 'tremendous', 'job', 'crafting', 'world', 'building', 'setting'], ['tried', 'read', 'sanderson', 'book', 'first', 'actually', 'followed', 'way'], ['looking', 'forward', 'cornell', 'next', 'book', 'group'], ['since', 'certain', 'mile', 'better', 'lot', 'thriller'], ['every', 'time', 'reread', 'ellen', 'hopkins', 'novel', 'notice', 'something', 'new', 'formatting', 'notice', 'maybe', 'notice', 'mention'], ['must', 'read'], ['slowly', 'two', 'develop', 'bond', 'beautiful', 'lasting', 'quite', 'magical'], ['hill', 'poured', 'much', 'care', 'attention', 'book', 'guess', 'really', 'matter', 'dedicated'], ['took', 'get', 'hour', 'audiobook', 'took', 'long', 'crime', 'committed', 'anxious', 'nice', 'bloody', 'murder'], ['star', 'rounded', 'goodreads', 'yet', 'add', 'half', 'star'], ['think', 'hill', 'played', 'devilish', 'reference', 'bit', 'much', 'evil', 'knieval', 'trail', 'number', 'time', 'throw', 'book', 'still', 'fun', 'read', 'well', 'worth', 'price', 'admission'], ['needed', 'break', 'usual', 'contemporary', 'romance', 'thriller', 'kept', 'glued', 'kindle', 'rachel', 'see', 'perfect', 'couple', 'train', 'every', 'day', 'concocts', 'elaborate', 'fantasy', 'life'], ['gaiman', 'read', 'wonderful', 'job', 'voice', 'soothing', 'pleasant', 'listen', 'terrific', 'grasp', 'character', 'voice', 'inflection', 'utmost', 'importance', 'audiobooks'], ['also', 'liked', 'crawford', 'able', 'feed', 'reader', 'bit', 'piece', 'background', 'without', 'sounding', 'matter', 'fact'], ['character', 'interesting', 'fairly', 'well', 'developed'], ['good', 'little', 'read', 'quirky', 'meet', 'mystery', 'work', 'well', 'together'], ['coming', 'paula', 'hawkins', 'new', 'release', 'say', 'writer', 'unsettling', 'dark', 'humanity', 'unreliable', 'narrator', 'better', 'megan', 'abbott'], ['able', 'take', 'important', 'theme', 'write', 'using', 'well', 'developed', 'interesting', 'character'], ['glad', 'part', 'three'], ['remained', 'involved']]\n"
     ]
    }
   ],
   "source": [
    "print(texts_pos_ppr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f21cc91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates the vocabulary of the corpus (the vocabulary of all the positive sentiments in the dataset) \n",
    "# and associates it with a unique id. Words are represented by unique ids.\n",
    "id2word = corpora.Dictionary(texts_pos_ppr)\n",
    "\n",
    "# create a Bag-of-words of each text in texts_pos\n",
    "corpus_pos = [id2word.doc2bow(text) for text in texts_pos_ppr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "727919fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)], [(5, 1), (6, 1), (7, 1), (8, 1), (9, 1)], [(0, 1), (10, 1), (11, 1)], [(8, 1), (12, 1), (13, 1)], [(14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1)]]\n"
     ]
    }
   ],
   "source": [
    "print(corpus_pos[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b619960c",
   "metadata": {},
   "source": [
    "Prints the first five elements of the list.\n",
    "\n",
    "The first number is the index of the word in the vocabulary and the second number is the frequency of this word in the specific document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "91825367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('book', 1), ('like', 1), ('listened', 1), ('stephen', 1), ('steven', 1)], [('action', 1), ('moving', 1), ('plot', 1), ('quick', 1), ('violent', 1)], [('book', 1), ('everything', 1), ('loved', 1)]]\n"
     ]
    }
   ],
   "source": [
    "# see the actual words\n",
    "print([[(id2word[id], freq) for id, freq in cp] for cp in corpus_pos[:3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4ede9e",
   "metadata": {},
   "source": [
    "Train an LDA model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec039f11",
   "metadata": {},
   "source": [
    "Use corpus_pos for topic modelling, which was generated from the positive texts after removing stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312e3a99",
   "metadata": {},
   "source": [
    "passes: total number of training passes\n",
    "\n",
    "chunksize: number of documents processed at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc631eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for 'num_topics' parameter , create an LDA model with 4 topics\n",
    "# for 'passes' parameter, perform 100 passes over the corpus during training\n",
    "# for 'chunksize' parameter, processing each chunk of 20 documents\n",
    "\n",
    "lda_model_pos_100p = gensim.models.ldamodel.LdaModel(\n",
    "   corpus=corpus_pos, id2word=id2word, num_topics=4, random_state=42, \n",
    "   update_every=1, chunksize=10, passes=100, alpha='auto', per_word_topics=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "28b55741",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.014*\"world\" + 0.014*\"series\" + 0.013*\"well\" + 0.013*\"developed\" + '\n",
      "  '0.011*\"could\" + 0.010*\"hero\" + 0.010*\"many\" + 0.010*\"one\" + '\n",
      "  '0.010*\"crawford\" + 0.008*\"part\"'),\n",
      " (1,\n",
      "  '0.037*\"character\" + 0.024*\"good\" + 0.021*\"novel\" + 0.020*\"great\" + '\n",
      "  '0.015*\"lot\" + 0.015*\"reader\" + 0.014*\"mystery\" + 0.012*\"liked\" + '\n",
      "  '0.012*\"better\" + 0.012*\"story\"'),\n",
      " (2,\n",
      "  '0.059*\"book\" + 0.025*\"read\" + 0.016*\"much\" + 0.015*\"really\" + 0.015*\"love\" '\n",
      "  '+ 0.011*\"think\" + 0.011*\"story\" + 0.011*\"first\" + 0.010*\"time\" + '\n",
      "  '0.009*\"way\"'),\n",
      " (3,\n",
      "  '0.032*\"read\" + 0.023*\"well\" + 0.012*\"bit\" + 0.012*\"hill\" + 0.010*\"little\" + '\n",
      "  '0.010*\"take\" + 0.010*\"fun\" + 0.010*\"voice\" + 0.009*\"see\" + 0.009*\"star\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(lda_model_pos_100p.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "364cc8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The output is stored in the variable doc_lda_pos\n",
    "doc_lda_pos = lda_model_pos_100p[corpus_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1dd1beb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jo/opt/anaconda3/lib/python3.9/site-packages/pyLDAvis/_prepare.py:243: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el980601402020822843689807312783\" style=\"background-color:white;\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el980601402020822843689807312783_data = {\"mdsDat\": {\"x\": [-0.18428524484844264, 0.09593371761575746, 0.051225257733506425, 0.037126269499178785], \"y\": [0.025117002930464254, 0.1304104814537178, -0.14950817265792066, -0.006019311726261456], \"topics\": [1, 2, 3, 4], \"cluster\": [1, 1, 1, 1], \"Freq\": [32.88009821003856, 25.150715557881004, 23.324272307897402, 18.644913924183033]}, \"tinfo\": {\"Term\": [\"book\", \"character\", \"read\", \"good\", \"well\", \"novel\", \"great\", \"much\", \"lot\", \"mystery\", \"reader\", \"love\", \"world\", \"series\", \"liked\", \"developed\", \"bit\", \"better\", \"hill\", \"could\", \"really\", \"think\", \"story\", \"hero\", \"little\", \"first\", \"take\", \"many\", \"voice\", \"crawford\", \"book\", \"love\", \"much\", \"think\", \"first\", \"way\", \"loved\", \"make\", \"writer\", \"notice\", \"would\", \"took\", \"tale\", \"page\", \"enjoy\", \"work\", \"another\", \"must\", \"next\", \"thought\", \"tell\", \"actually\", \"enough\", \"ride\", \"tension\", \"guess\", \"bring\", \"writing\", \"bloody\", \"excellent\", \"really\", \"reading\", \"time\", \"new\", \"read\", \"story\", \"enjoyed\", \"every\", \"character\", \"good\", \"great\", \"novel\", \"lot\", \"mystery\", \"reader\", \"liked\", \"better\", \"girl\", \"two\", \"beautiful\", \"humanity\", \"may\", \"background\", \"although\", \"narrator\", \"human\", \"paced\", \"together\", \"mind\", \"fairly\", \"remained\", \"soon\", \"exciting\", \"five\", \"suspense\", \"adventure\", \"line\", \"main\", \"story\", \"fun\", \"interesting\", \"also\", \"like\", \"bit\", \"hill\", \"little\", \"voice\", \"take\", \"star\", \"see\", \"fact\", \"dark\", \"evil\", \"able\", \"matter\", \"perfect\", \"thick\", \"devil\", \"needed\", \"usual\", \"without\", \"written\", \"coming\", \"right\", \"forward\", \"rachel\", \"admission\", \"devilish\", \"knieval\", \"number\", \"played\", \"price\", \"reference\", \"even\", \"well\", \"read\", \"fun\", \"world\", \"series\", \"developed\", \"hero\", \"could\", \"many\", \"part\", \"crawford\", \"pure\", \"meet\", \"point\", \"put\", \"turn\", \"building\", \"far\", \"glad\", \"three\", \"theme\", \"awesome\", \"create\", \"deserves\", \"try\", \"twist\", \"felt\", \"third\", \"crafting\", \"done\", \"setting\", \"tremendous\", \"absolutely\", \"magic\", \"one\", \"creative\", \"well\", \"like\", \"get\"], \"Freq\": [30.0, 14.0, 24.0, 9.0, 12.0, 8.0, 8.0, 8.0, 6.0, 6.0, 6.0, 7.0, 4.0, 4.0, 5.0, 4.0, 4.0, 5.0, 4.0, 3.0, 9.0, 6.0, 10.0, 3.0, 4.0, 5.0, 4.0, 3.0, 3.0, 3.0, 29.815221437701602, 7.416459934499675, 8.352295483989526, 5.6850716549532585, 5.420315096409927, 4.6964553328756535, 4.425653459783309, 4.130749605120015, 4.496083191035308, 4.066180209625115, 3.9610612922731163, 4.153007933272573, 3.5934973434497044, 3.546099595272057, 3.2691946017582247, 3.5980378177757215, 3.0701128285568826, 3.2548809239645142, 2.761655502308485, 2.927476720258095, 2.6728168421259695, 2.695542807989719, 2.4735611794225347, 2.425136610195894, 2.616785491966529, 2.5105215464548882, 2.413739179682872, 2.3778889213443404, 2.2017862801339465, 2.1932579084342883, 7.5516100643420145, 3.146567111432273, 5.081019634879929, 3.478591773916825, 12.483324916720779, 5.561651162433909, 2.8941892675799874, 2.731938565010528, 14.192258420315838, 9.11378847848427, 7.853387619039825, 8.13942066589264, 5.828360536419337, 5.61350349216461, 5.620855063481908, 4.786217639624649, 4.642598321279998, 3.269523541054339, 2.974845260017767, 2.7839807168518163, 2.676149549376368, 2.4076930277609545, 2.351907426195269, 2.4627881223334653, 2.161261621784082, 2.0738612315316747, 2.1623867186464585, 2.099933736043435, 2.0534423658249823, 1.9589766416026833, 1.8283726401215417, 1.8834444530939167, 1.682235299411816, 1.815773504420149, 1.8137261674283627, 1.6140036070820643, 1.5796053135977117, 1.5719229269854615, 4.588365187345072, 2.890976634362703, 2.053870278025928, 1.9954444708138355, 2.0183972409073996, 4.44648384934285, 4.158393733137683, 3.616483434713926, 3.4415684223028333, 3.6105250180289175, 3.1909584207589194, 3.2277044450948797, 2.784034713363982, 2.9146160929560194, 2.3708143319772828, 2.426003831779661, 2.333589123737694, 2.2482850161015104, 2.2757951598331974, 2.1151920634539887, 2.085228926606136, 2.085228926606136, 2.0794869209540976, 2.0793456596147113, 2.3136018158133402, 1.9595624039229542, 2.111280764591293, 1.8872580216779107, 1.884088679589503, 1.884088679589503, 1.884088679589503, 1.884088679589503, 1.884088679589503, 1.884088679589503, 1.884088679589503, 2.1527235610905877, 8.170048400449133, 11.431100563914455, 3.540123981697973, 4.017666179166757, 3.9320799145386074, 3.6016105837526693, 2.961214755674936, 3.231330364980491, 2.8364418808547893, 2.437490105556528, 2.7684108536758942, 2.0879507338094574, 2.3217776877072644, 1.991042108997935, 1.8615039315207893, 1.8353167758436773, 1.8609470853614152, 1.7396120197832479, 1.7212086689740043, 1.681742297773057, 1.7631835237506617, 1.4646713023826259, 1.4553057975397163, 1.4163459663578442, 1.3670894315658246, 1.3638892385610024, 1.3739303976818646, 1.439634632079002, 1.274419634246203, 1.274419634246203, 1.274419634246203, 1.274419634246203, 1.2752941048126782, 2.2345452349135835, 2.8303797233629004, 1.2923878904590305, 3.7543073802265825, 2.1450017769034515, 1.8896603852805127], \"Total\": [30.0, 14.0, 24.0, 9.0, 12.0, 8.0, 8.0, 8.0, 6.0, 6.0, 6.0, 7.0, 4.0, 4.0, 5.0, 4.0, 4.0, 5.0, 4.0, 3.0, 9.0, 6.0, 10.0, 3.0, 4.0, 5.0, 4.0, 3.0, 3.0, 3.0, 30.31312305692228, 7.896249745761168, 8.89941615708019, 6.188855258672395, 5.9126409170358984, 5.167454737442108, 4.89792567267038, 4.600336351255209, 5.017336113136778, 4.538333581833206, 4.431813225704707, 4.739078862533068, 4.103473606683261, 4.051017627797501, 3.741282870368691, 4.127452603509422, 3.551308373633095, 3.797557429771646, 3.231334290090875, 3.433188387702409, 3.142932645688069, 3.206072339660315, 2.9467106905118206, 2.893246919395384, 3.1286738357531703, 3.00749821349412, 2.8924294306144387, 2.8503071421584236, 2.669887958235889, 2.6692320226033415, 9.254153068998113, 3.860599944600196, 6.834549807629112, 4.453889143150812, 24.260858433383337, 10.473324904720084, 5.077483187184128, 4.9184272499006605, 14.730163892255916, 9.631305408436397, 8.35020159105318, 8.749384941495263, 6.318430259973365, 6.0926585134404965, 6.131466221591008, 5.273653588169645, 5.142578764743335, 3.779808502322437, 3.45006047216503, 3.2589887360938605, 3.1535880770834144, 2.8842503838588156, 2.861913081988062, 3.00005039684124, 2.64136861546366, 2.548732894441571, 2.6576051983825546, 2.594205628650181, 2.545416156049622, 2.4497373197525443, 2.3137027947252955, 2.4062888309369797, 2.1697899008175296, 2.347658814789103, 2.3453998772111873, 2.090646931504888, 2.054478151933598, 2.0473435181891038, 10.473324904720084, 6.816956299011618, 4.106148484469808, 3.98235902882197, 5.304108824322208, 4.97646676669288, 4.6671653670984705, 4.129193840163335, 3.946523570318278, 4.150755176478335, 3.6966847985362485, 3.7745196155174443, 3.2897297083013592, 3.478518744585417, 2.875106955416719, 2.9628246883274403, 2.8556674414069136, 2.7531681094691436, 2.787177289570284, 2.617206061725863, 2.5880153550229585, 2.5880153550229585, 2.5889658355517935, 2.5952214200479, 2.9001428824425886, 2.4618385467998207, 2.6569992297134077, 2.390290890985146, 2.3910181260780186, 2.3910181260780186, 2.3910181260780186, 2.3910181260780186, 2.3910181260780186, 2.3910181260780186, 2.3910181260780186, 2.7330038974085267, 12.305844131701377, 24.260858433383337, 6.816956299011618, 4.524684637071484, 4.4333616491417445, 4.140297097241018, 3.4770868046541827, 3.822578078520043, 3.3972710620546223, 2.929879473494568, 3.425564130497001, 2.5881117124257544, 2.90283624204114, 2.491163034910162, 2.3552427880729505, 2.3294536410603204, 2.3780767888529133, 2.2337312725437863, 2.2314350589521394, 2.2025835195913244, 2.3454086469636994, 1.9569547435479264, 1.9552910810510733, 1.910942176836716, 1.8595148095219582, 1.8598464814872, 1.8878251583231602, 1.993514839736585, 1.7666992646990853, 1.7666992646990853, 1.7666992646990853, 1.7666992646990853, 1.7751539384532182, 3.2815484841258424, 4.738260406521587, 1.8100825816146302, 12.305844131701377, 5.304108824322208, 4.146971618669959], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -2.8329, -4.2242, -4.1053, -4.49, -4.5377, -4.6811, -4.7405, -4.8094, -4.7247, -4.8252, -4.8514, -4.8041, -4.9488, -4.962, -5.0433, -4.9475, -5.1062, -5.0477, -5.2121, -5.1537, -5.2448, -5.2363, -5.3222, -5.342, -5.2659, -5.3074, -5.3467, -5.3617, -5.4386, -5.4425, -4.2061, -5.0816, -4.6024, -4.9813, -3.7035, -4.512, -5.1652, -5.2229, -3.3072, -3.7501, -3.899, -3.8632, -4.1972, -4.2347, -4.2334, -4.3942, -4.4246, -4.7753, -4.8697, -4.936, -4.9755, -5.0812, -5.1047, -5.0586, -5.1892, -5.2305, -5.1887, -5.218, -5.2404, -5.2875, -5.3565, -5.3268, -5.4398, -5.3634, -5.3645, -5.4812, -5.5027, -5.5076, -4.4364, -4.8983, -5.2402, -5.269, -5.2576, -4.3924, -4.4594, -4.599, -4.6486, -4.6007, -4.7242, -4.7127, -4.8606, -4.8148, -5.0213, -4.9983, -5.0371, -5.0743, -5.0622, -5.1354, -5.1496, -5.1496, -5.1524, -5.1525, -5.0457, -5.2118, -5.1372, -5.2494, -5.2511, -5.2511, -5.2511, -5.2511, -5.2511, -5.2511, -5.2511, -5.1178, -3.784, -3.4482, -4.6203, -4.2699, -4.2914, -4.3792, -4.575, -4.4877, -4.618, -4.7696, -4.6423, -4.9244, -4.8183, -4.9719, -5.0392, -5.0534, -5.0395, -5.1069, -5.1176, -5.1408, -5.0935, -5.279, -5.2854, -5.3125, -5.3479, -5.3502, -5.3429, -5.2962, -5.4181, -5.4181, -5.4181, -5.4181, -5.4174, -4.8566, -4.6202, -5.4041, -4.3377, -4.8974, -5.0242], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.0957, 1.0496, 1.0489, 1.0274, 1.0254, 1.0167, 1.0109, 1.0046, 1.0026, 1.0024, 1.0, 0.9803, 0.9796, 0.9792, 0.9774, 0.975, 0.9667, 0.9581, 0.9552, 0.953, 0.9503, 0.9389, 0.9373, 0.9358, 0.9336, 0.9317, 0.9314, 0.9311, 0.9195, 0.9159, 0.909, 0.9078, 0.8158, 0.8652, 0.4478, 0.4794, 0.5502, 0.5243, 1.3431, 1.3251, 1.3189, 1.308, 1.2995, 1.2984, 1.2933, 1.2833, 1.278, 1.2353, 1.2321, 1.2227, 1.2161, 1.1997, 1.184, 1.1829, 1.1797, 1.1741, 1.1741, 1.1689, 1.1655, 1.1567, 1.1449, 1.1353, 1.1258, 1.1234, 1.1232, 1.1215, 1.1174, 1.116, 0.555, 0.5225, 0.6875, 0.6893, 0.4141, 1.3431, 1.3403, 1.3231, 1.3188, 1.3162, 1.3086, 1.2992, 1.2888, 1.2788, 1.2628, 1.2558, 1.2538, 1.2531, 1.253, 1.2427, 1.2397, 1.2397, 1.2365, 1.2341, 1.2297, 1.2275, 1.2258, 1.2194, 1.2174, 1.2174, 1.2174, 1.2174, 1.2174, 1.2174, 1.2174, 1.217, 1.0461, 0.7031, 0.8004, 1.5608, 1.5596, 1.5402, 1.519, 1.5116, 1.4992, 1.4956, 1.4666, 1.4649, 1.4562, 1.4555, 1.4443, 1.4412, 1.4344, 1.4296, 1.42, 1.4098, 1.3943, 1.3898, 1.3843, 1.3801, 1.372, 1.3694, 1.3618, 1.3541, 1.353, 1.353, 1.353, 1.353, 1.3489, 1.2953, 1.1643, 1.3427, 0.4924, 0.7743, 0.8936]}, \"token.table\": {\"Topic\": [3, 4, 1, 3, 2, 1, 2, 2, 1, 4, 2, 2, 2, 3, 1, 1, 1, 4, 2, 3, 4, 4, 4, 4, 4, 3, 4, 4, 3, 3, 4, 1, 1, 3, 1, 3, 1, 3, 3, 1, 2, 3, 2, 4, 4, 1, 2, 3, 2, 3, 1, 4, 2, 4, 2, 2, 1, 4, 3, 2, 2, 2, 3, 3, 2, 3, 4, 2, 2, 3, 2, 1, 1, 3, 4, 2, 1, 4, 3, 2, 4, 2, 1, 1, 2, 2, 3, 1, 4, 1, 1, 2, 3, 1, 4, 2, 1, 4, 3, 3, 4, 3, 4, 4, 3, 1, 3, 2, 1, 1, 3, 3, 2, 1, 3, 3, 4, 4, 2, 3, 1, 2, 2, 3, 1, 1, 1, 4, 3, 1, 4, 1, 4, 1, 2, 2, 1, 4, 4, 4, 4, 2, 3, 3, 1, 3, 4, 3, 1, 4, 1, 1, 1, 3], \"Freq\": [0.6750315021605381, 0.5633314262713186, 0.9357243636984971, 0.8364637549948629, 0.9566416834239733, 0.5022148895981446, 0.5022148895981446, 0.6666554675567465, 0.8447590815468695, 0.5109980204176907, 0.6988332429057126, 0.9205309508359094, 0.9722748505631394, 0.8037831231530974, 0.749095104845331, 0.9896703795140377, 0.6914602578826409, 0.8410157356460798, 0.950430701409929, 0.6896211949100726, 0.7848106535371243, 0.56602729167396, 0.8757681613056657, 0.5114328038884352, 0.5524609817017188, 0.8624360597940521, 0.5233020716803441, 0.9661142439912082, 0.7641736847732734, 0.8364637549948629, 0.56602729167396, 0.801863987286361, 0.5908439062038019, 0.39389593746920126, 0.6787228914056085, 0.7317955169754528, 0.609951077361283, 0.40663405157418864, 0.6956262953042452, 0.7492791870709578, 0.9217482297463194, 0.9119290233570708, 0.816414063611533, 0.8953628507525833, 0.5297100717146067, 0.8456458070358482, 0.8519125468321792, 0.7527288595472142, 0.4400791010549629, 0.5867721347399506, 0.4822796449813785, 0.4822796449813785, 0.7936910026411927, 0.8962842059760326, 0.934452768169576, 0.9580607022196442, 0.9975068269498958, 0.8627912297111513, 0.8570512688918841, 0.7847036479819912, 0.9512973561133388, 0.4870744464220813, 0.4870744464220813, 0.8364637549948629, 0.37706617006591536, 0.18853308503295768, 0.37706617006591536, 0.9481092977393263, 0.9734832167076951, 0.9687120912303248, 0.9496029477462797, 0.8864967833315697, 0.8166722541992302, 0.30473418413209447, 0.6094683682641889, 0.9768756352959372, 0.8695016395722007, 0.8830617119452464, 0.7003616636167731, 0.6934210743951487, 0.6889813386764431, 0.7857261356838071, 0.8989353749498912, 0.7899814697944925, 0.9847917763261981, 0.7571832224745824, 0.7727929419422853, 0.6735686281310791, 0.22452287604369303, 0.9284090504655372, 0.8813807817062771, 0.9143499861411751, 0.8364637549948629, 0.42209583864307354, 0.6331437579646103, 0.7525572275435118, 0.9874062192552742, 0.6826219365312427, 0.7264358442629328, 0.8364637549948629, 0.802837860056849, 0.8364637549948629, 0.7727641702627527, 0.8491693553327432, 0.8367182452742019, 0.4946238828667251, 0.45340522596116467, 0.9785587628081404, 0.7770812938533262, 0.8644767317282022, 0.10805959146602527, 0.8364637549948629, 0.8644152587616417, 0.6912648853413278, 0.8124009604934608, 0.7948031287654955, 0.9022498764057205, 0.56602729167396, 0.8311554183714613, 0.8115379491342865, 0.5728839747247734, 0.4774033122706445, 0.8527330539379548, 0.9636800605989385, 0.9747838985695594, 0.9545225234514126, 0.9588727229145014, 0.852729865471053, 0.7175718629324624, 0.9694846218276387, 0.5016265643310365, 0.8738232981172607, 0.9080245912178111, 0.7315770812612583, 0.14631541625225167, 0.7709489093355493, 0.8440458823388253, 0.56602729167396, 0.5377746898703532, 0.8585704238740035, 0.5376787869073818, 0.8695499757769168, 0.7727929419422853, 0.7601626967498529, 0.9675943484848021, 0.6500976214537783, 0.3250488107268891, 0.7725092284092403, 0.9691207590364445, 0.8840395123291783, 0.9025651119049485, 0.7972358059741882, 0.7016787666207369, 0.7706471534760553], \"Term\": [\"able\", \"absolutely\", \"actually\", \"admission\", \"adventure\", \"also\", \"also\", \"although\", \"another\", \"awesome\", \"background\", \"beautiful\", \"better\", \"bit\", \"bloody\", \"book\", \"bring\", \"building\", \"character\", \"coming\", \"could\", \"crafting\", \"crawford\", \"create\", \"creative\", \"dark\", \"deserves\", \"developed\", \"devil\", \"devilish\", \"done\", \"enjoy\", \"enjoyed\", \"enjoyed\", \"enough\", \"even\", \"every\", \"every\", \"evil\", \"excellent\", \"exciting\", \"fact\", \"fairly\", \"far\", \"felt\", \"first\", \"five\", \"forward\", \"fun\", \"fun\", \"get\", \"get\", \"girl\", \"glad\", \"good\", \"great\", \"guess\", \"hero\", \"hill\", \"human\", \"humanity\", \"interesting\", \"interesting\", \"knieval\", \"like\", \"like\", \"like\", \"liked\", \"line\", \"little\", \"lot\", \"love\", \"loved\", \"magic\", \"magic\", \"main\", \"make\", \"many\", \"matter\", \"may\", \"meet\", \"mind\", \"much\", \"must\", \"mystery\", \"narrator\", \"needed\", \"new\", \"new\", \"next\", \"notice\", \"novel\", \"number\", \"one\", \"one\", \"paced\", \"page\", \"part\", \"perfect\", \"played\", \"point\", \"price\", \"pure\", \"put\", \"rachel\", \"read\", \"read\", \"reader\", \"reading\", \"really\", \"really\", \"reference\", \"remained\", \"ride\", \"right\", \"see\", \"series\", \"setting\", \"soon\", \"star\", \"story\", \"story\", \"suspense\", \"take\", \"tale\", \"tell\", \"tension\", \"theme\", \"thick\", \"think\", \"third\", \"thought\", \"three\", \"time\", \"time\", \"together\", \"took\", \"tremendous\", \"try\", \"turn\", \"twist\", \"two\", \"usual\", \"voice\", \"way\", \"well\", \"well\", \"without\", \"work\", \"world\", \"would\", \"writer\", \"writing\", \"written\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [3, 2, 4, 1]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el980601402020822843689807312783\", ldavis_el980601402020822843689807312783_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el980601402020822843689807312783\", ldavis_el980601402020822843689807312783_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el980601402020822843689807312783\", ldavis_el980601402020822843689807312783_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "2     -0.184285  0.025117       1        1  32.880098\n",
       "1      0.095934  0.130410       2        1  25.150716\n",
       "3      0.051225 -0.149508       3        1  23.324272\n",
       "0      0.037126 -0.006019       4        1  18.644914, topic_info=          Term       Freq      Total Category  logprob  loglift\n",
       "0         book  30.000000  30.000000  Default  30.0000  30.0000\n",
       "16   character  14.000000  14.000000  Default  29.0000  29.0000\n",
       "13        read  24.000000  24.000000  Default  28.0000  28.0000\n",
       "45        good   9.000000   9.000000  Default  27.0000  27.0000\n",
       "36        well  12.000000  12.000000  Default  26.0000  26.0000\n",
       "..         ...        ...        ...      ...      ...      ...\n",
       "400        one   2.830380   4.738260   Topic4  -4.6202   1.1643\n",
       "421   creative   1.292388   1.810083   Topic4  -5.4041   1.3427\n",
       "36        well   3.754307  12.305844   Topic4  -4.3377   0.4924\n",
       "1         like   2.145002   5.304109   Topic4  -4.8974   0.7743\n",
       "287        get   1.889660   4.146972   Topic4  -5.0242   0.8936\n",
       "\n",
       "[173 rows x 6 columns], token_table=      Topic      Freq        Term\n",
       "term                             \n",
       "820       3  0.675032        able\n",
       "161       4  0.563331  absolutely\n",
       "445       1  0.935724    actually\n",
       "590       3  0.836464   admission\n",
       "87        2  0.956642   adventure\n",
       "...     ...       ...         ...\n",
       "112       4  0.884040       world\n",
       "201       1  0.902565       would\n",
       "323       1  0.797236      writer\n",
       "141       1  0.701679     writing\n",
       "233       3  0.770647     written\n",
       "\n",
       "[154 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[3, 2, 4, 1])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model_pos_100p, corpus_pos, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "efd3acf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(vis, 'lda_visualization_pos.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ba8433",
   "metadata": {},
   "source": [
    "### Topic Model for the Negative Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "09b048d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2     Mild, because it isn't conclusive, and doesn't...\n",
       "12    Going in I really liked it but unfortunately l...\n",
       "14    I'm not giving this 5 stars because the big re...\n",
       "17    eh I hate how the author made Duke from a nice...\n",
       "21    I guess I didn't track this on goodreads when ...\n",
       "Name: texts, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a new dataframe with only negative sentiment\n",
    "\n",
    "texts_neg = df.loc[df['sentiment'] == -1]['texts']\n",
    "texts_neg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb878186",
   "metadata": {},
   "source": [
    "#### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "316486c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove non-alphabetic characters\n",
    "\n",
    "texts_neg_clean = remove_non_alphabetic(texts_neg)\n",
    "\n",
    "# remove stop words from each negativetext\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "texts_neg_nostops = remove_stopwords(texts_neg_clean)\n",
    "\n",
    "# reduce inflected words to their root word\n",
    "\n",
    "texts_neg_lemmatized = lemmatize(texts_neg_nostops)\n",
    "\n",
    "texts_neg_ppr = texts_neg_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3e55bfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(texts_neg_ppr)\n",
    "corpus_neg = [id2word.doc2bow(text) for text in texts_neg_ppr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c96f5b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('appreciate', 1), ('conclusive', 1), ('fully', 1), ('give', 1), ('information', 1), ('lloyd', 1), ('mild', 1), ('need', 1), ('story', 1), ('telling', 1), ('u', 1)], [('book', 1), ('bored', 1), ('end', 1), ('finished', 1), ('first', 1), ('going', 1), ('hard', 1), ('infuriated', 1), ('left', 1), ('liked', 1), ('really', 2), ('think', 1), ('unfortunately', 1), ('wanted', 1)], [('really', 1), ('big', 1), ('case', 1), ('classic', 1), ('disappointed', 1), ('giving', 1), ('kind', 1), ('possible', 1), ('probable', 1), ('reveal', 1), ('star', 1)]]\n"
     ]
    }
   ],
   "source": [
    "# see the actual words\n",
    "print([[(id2word[id], freq) for id, freq in cp] for cp in corpus_neg[:3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "245d806c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for 'num_topics' parameter , create an LDA model with 4 topics\n",
    "# for 'passes' parameter, perform 100 passes over the corpus during training\n",
    "# for 'chunksize' parameter, processing each chunk of 5 documents\n",
    "\n",
    "lda_model_neg_100p = gensim.models.ldamodel.LdaModel(\n",
    "   corpus=corpus_neg, id2word=id2word, num_topics=4, random_state=42, \n",
    "   update_every=1, chunksize=5, passes=100, alpha='auto', per_word_topics=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6c99ea34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.034*\"character\" + 0.022*\"felt\" + 0.021*\"feel\" + 0.018*\"made\" + '\n",
      "  '0.016*\"author\" + 0.016*\"every\" + 0.013*\"scene\" + 0.012*\"even\" + '\n",
      "  '0.012*\"cheesier\" + 0.010*\"one\"'),\n",
      " (1,\n",
      "  '0.061*\"book\" + 0.037*\"first\" + 0.022*\"part\" + 0.021*\"like\" + 0.020*\"sequel\" '\n",
      "  '+ 0.018*\"unfortunately\" + 0.017*\"overlooked\" + 0.017*\"largely\" + '\n",
      "  '0.017*\"ignored\" + 0.017*\"flaw\"'),\n",
      " (2,\n",
      "  '0.020*\"get\" + 0.018*\"interesting\" + 0.017*\"much\" + 0.014*\"reading\" + '\n",
      "  '0.013*\"place\" + 0.013*\"got\" + 0.013*\"though\" + 0.011*\"sort\" + 0.009*\"found\" '\n",
      "  '+ 0.009*\"lee\"'),\n",
      " (3,\n",
      "  '0.017*\"disappointed\" + 0.016*\"story\" + 0.015*\"time\" + 0.015*\"really\" + '\n",
      "  '0.014*\"got\" + 0.013*\"good\" + 0.013*\"something\" + 0.011*\"much\" + '\n",
      "  '0.010*\"wanted\" + 0.010*\"finished\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(lda_model_neg_100p.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3d0fd119",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jo/opt/anaconda3/lib/python3.9/site-packages/pyLDAvis/_prepare.py:243: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el980601402014234097765208497423\" style=\"background-color:white;\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el980601402014234097765208497423_data = {\"mdsDat\": {\"x\": [-0.06989824524623572, -0.06516216962973818, -0.04034248890631169, 0.17540290378228568], \"y\": [0.09906657561728698, -0.12302428730462812, 0.02453911905919551, -0.0005814073718543785], \"topics\": [1, 2, 3, 4], \"cluster\": [1, 1, 1, 1], \"Freq\": [28.187285956619405, 25.500343313801615, 23.306665148873595, 23.00570558070538]}, \"tinfo\": {\"Term\": [\"book\", \"first\", \"character\", \"part\", \"felt\", \"like\", \"feel\", \"sequel\", \"get\", \"unfortunately\", \"made\", \"interesting\", \"disappointed\", \"ignored\", \"largely\", \"pevel\", \"overlooked\", \"revelation\", \"stronger\", \"thanks\", \"certain\", \"flaw\", \"justice\", \"cast\", \"killer\", \"every\", \"author\", \"much\", \"really\", \"story\", \"get\", \"interesting\", \"place\", \"reading\", \"sort\", \"though\", \"found\", \"lee\", \"started\", \"obsession\", \"xd\", \"honest\", \"said\", \"explanation\", \"world\", \"dealt\", \"detail\", \"different\", \"done\", \"fascination\", \"husband\", \"ohlsson\", \"relationship\", \"seems\", \"study\", \"unfaithful\", \"way\", \"connected\", \"echo\", \"experience\", \"would\", \"much\", \"got\", \"make\", \"enough\", \"character\", \"felt\", \"feel\", \"made\", \"every\", \"author\", \"scene\", \"cheesier\", \"one\", \"character\", \"guy\", \"right\", \"absence\", \"cell\", \"dropping\", \"fix\", \"highlight\", \"phone\", \"reference\", \"simply\", \"someone\", \"technology\", \"grasp\", \"introduced\", \"language\", \"new\", \"question\", \"speaking\", \"whether\", \"cait\", \"confusing\", \"even\", \"love\", \"time\", \"disappointed\", \"really\", \"something\", \"wanted\", \"finished\", \"story\", \"explaining\", \"reader\", \"fully\", \"information\", \"fresh\", \"mind\", \"development\", \"island\", \"lexie\", \"oaksey\", \"possibly\", \"spent\", \"villager\", \"wife\", \"aggravated\", \"finally\", \"happy\", \"mild\", \"kind\", \"cliffhanger\", \"ending\", \"honestly\", \"supposedly\", \"build\", \"good\", \"time\", \"got\", \"conclusion\", \"much\", \"book\", \"first\", \"part\", \"like\", \"sequel\", \"unfortunately\", \"certain\", \"flaw\", \"ignored\", \"largely\", \"overlooked\", \"pevel\", \"revelation\", \"stronger\", \"thanks\", \"cast\", \"justice\", \"killer\", \"page\", \"left\", \"track\", \"short\", \"end\", \"last\", \"read\", \"lot\", \"unexplained\", \"little\", \"writing\", \"low\", \"sure\", \"star\"], \"Freq\": [8.0, 5.0, 6.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 4.0, 2.0, 2.0, 3.117202062511161, 2.8105361477394077, 1.9964573255244638, 2.0699624621591632, 1.6910446920018427, 1.9478781823793143, 1.4178404368042432, 1.3469532843465564, 1.3260859601753976, 1.2761019966277036, 1.2741589216269287, 1.2525724598268992, 1.1166082426026152, 1.112854848837895, 1.1112814783386173, 1.0775891078456805, 1.0775891078456805, 1.0775891078456805, 1.0775891078456805, 1.0775891078456805, 1.0775891078456805, 1.0775891078456805, 1.0775891078456805, 1.0775891078456805, 1.0775891078456805, 1.0775891078456805, 1.0775891078456805, 1.077588611693167, 1.077588611693167, 1.077588611693167, 1.1764056067150677, 2.653902783833648, 1.9702427530783921, 1.3334959270857725, 1.156574745144636, 1.2515206164982533, 2.973408802468093, 2.8301746998045223, 2.5139624110494685, 2.198414174102659, 2.251775058746376, 1.7900791244987801, 1.6355251058425264, 1.34148378591893, 4.679136803734813, 1.1963634580727294, 1.0869622692353442, 0.9721385677324333, 0.9721385677324333, 0.9721385677324333, 0.9721385677324333, 0.9721385677324333, 0.9721385677324333, 0.9721385677324333, 0.9721385677324333, 0.9721385677324333, 0.9721385677324333, 0.9721384394875882, 0.9721384394875882, 0.9721384394875882, 0.9721384394875882, 0.9721384394875882, 0.9721384394875882, 0.9721384394875882, 0.8948686759932128, 0.8948686759932128, 1.6355256188219072, 0.9315946652813882, 0.9721384394875882, 2.1811432019972554, 1.8528264615449626, 1.6232862408316997, 1.3097401541030749, 1.2292437483399061, 2.0230574955500695, 1.1619594213371975, 1.1619594213371975, 1.0941483412409598, 1.0941483412409598, 1.087249211938529, 1.087249211938529, 0.9995095408954927, 0.9995095408954927, 0.9995095408954927, 0.9995095408954927, 0.9995095408954927, 0.9995095408954927, 0.9995095408954927, 0.9995095408954927, 0.9172323838253722, 0.9172323838253722, 0.9172323838253722, 0.8500017987660066, 0.8494872943814836, 0.7829704750014783, 0.7829704750014783, 0.7829704750014783, 0.7829704750014783, 0.781061669009462, 1.6546063655793026, 1.9282380707014588, 1.783765427067462, 1.087249211938529, 1.4065368370591054, 7.5844438754289145, 4.5579312942280135, 2.6770117142549945, 2.5734012147857634, 2.5173848700768633, 2.293359754478646, 2.059253325874155, 2.059253325874155, 2.059253325874155, 2.059253325874155, 2.059253325874155, 2.059253325874155, 2.059253325874155, 2.059253325874155, 2.059253325874155, 2.0592524002824915, 2.0592524002824915, 2.0592524002824915, 1.8764861116014604, 0.9850672918450574, 0.8769269467722548, 0.9087448544156916, 0.8711997326523528, 0.9025813973746549, 0.8656717521381249, 0.7509747471155265, 0.7509747471155265, 2.0592524002824915, 2.059253325874155, 0.6822151456241938, 1.0287457310708743, 0.8772620688040985], \"Total\": [8.0, 5.0, 6.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 4.0, 2.0, 2.0, 3.5911600848061664, 3.245031298625399, 2.4308896963362145, 2.587860055961473, 2.128605565233948, 2.506335882904534, 1.8514970404181192, 1.7798249474539074, 1.7633940611031518, 1.7103660243827223, 1.709902551043946, 1.6858307526959582, 1.549931464280666, 1.5494781177873596, 1.549196806883364, 1.5104607709530316, 1.5104607709530316, 1.5104607709530316, 1.5104607709530316, 1.5104607709530316, 1.5104607709530316, 1.5104607709530316, 1.5104607709530316, 1.5104607709530316, 1.5104607709530316, 1.5104607709530316, 1.5104607709530316, 1.510460274800518, 1.510460274800518, 1.510460274800518, 1.649025434580508, 4.3377955817619505, 4.032433742661407, 2.393180242899674, 2.1341181176740567, 6.275483875482294, 3.461123770589384, 3.2998103983579474, 2.9497679363167877, 2.6342196993699782, 2.755780478251012, 2.2258846497660993, 2.071330631109846, 1.777619920958731, 6.275483875482294, 1.632576279828129, 1.5657209455972665, 1.4079440929997524, 1.4079440929997524, 1.4079440929997524, 1.4079440929997524, 1.4079440929997524, 1.4079440929997524, 1.4079440929997524, 1.4079440929997524, 1.4079440929997524, 1.4079440929997524, 1.4079439647549072, 1.4079439647549072, 1.4079439647549072, 1.4079439647549072, 1.4079439647549072, 1.4079439647549072, 1.4079439647549072, 1.330674201260532, 1.330674201260532, 2.825997492504075, 1.70908234913547, 3.1775620010561343, 2.6324912988771514, 2.2861633132020436, 2.0553963934780346, 1.742338752141265, 1.6628403391546378, 2.7723424363862867, 1.594501665579839, 1.594501665579839, 1.526142721363453, 1.526142721363453, 1.5187119209941955, 1.5187119209941955, 1.4309722499511592, 1.4309722499511592, 1.4309722499511592, 1.4309722499511592, 1.4309722499511592, 1.4309722499511592, 1.4309722499511592, 1.4309722499511592, 1.351485950769545, 1.351485950769545, 1.351485950769545, 1.2818538765167167, 1.2816705186980881, 1.2144331840571447, 1.2144331840571447, 1.2144331840571447, 1.2144331840571447, 1.2144110688766891, 2.6667313343724115, 3.1775620010561343, 4.032433742661407, 2.2593033787988293, 4.3377955817619505, 8.078485197701816, 5.039732931185815, 3.1710955187844085, 3.0568387825967998, 3.0059456444975927, 2.7656659541496267, 2.529361658811513, 2.529361658811513, 2.529361658811513, 2.529361658811513, 2.529361658811513, 2.529361658811513, 2.529361658811513, 2.529361658811513, 2.529361658811513, 2.5293607332198493, 2.5293607332198493, 2.5293607332198493, 2.4148873108228095, 1.4639661099663226, 1.347465188491804, 1.4056261093268532, 1.3493119083016598, 1.4149226220962925, 1.384955688152656, 1.2275090926011902, 1.2275090926011902, 3.4496527918269972, 3.4497391901561327, 1.1727715184193273, 1.9967762654742274, 1.659045967019492], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.8883, -3.9919, -4.3339, -4.2977, -4.4999, -4.3585, -4.6761, -4.7274, -4.743, -4.7815, -4.783, -4.8001, -4.915, -4.9183, -4.9198, -4.9505, -4.9505, -4.9505, -4.9505, -4.9505, -4.9505, -4.9505, -4.9505, -4.9505, -4.9505, -4.9505, -4.9505, -4.9505, -4.9505, -4.9505, -4.8628, -4.0492, -4.3471, -4.7375, -4.8798, -4.8009, -3.8354, -3.8848, -4.0032, -4.1374, -4.1134, -4.3428, -4.4331, -4.6313, -3.382, -4.7458, -4.8417, -4.9533, -4.9533, -4.9533, -4.9533, -4.9533, -4.9533, -4.9533, -4.9533, -4.9533, -4.9533, -4.9533, -4.9533, -4.9533, -4.9533, -4.9533, -4.9533, -4.9533, -5.0362, -5.0362, -4.4331, -4.9959, -4.9533, -4.0553, -4.2184, -4.3507, -4.5653, -4.6287, -4.1305, -4.685, -4.685, -4.7452, -4.7452, -4.7515, -4.7515, -4.8356, -4.8356, -4.8356, -4.8356, -4.8356, -4.8356, -4.8356, -4.8356, -4.9215, -4.9215, -4.9215, -4.9977, -4.9983, -5.0798, -5.0798, -5.0798, -5.0798, -5.0822, -4.3316, -4.1785, -4.2564, -4.7515, -4.494, -2.796, -3.3053, -3.8374, -3.8769, -3.8989, -3.9921, -4.0998, -4.0998, -4.0998, -4.0998, -4.0998, -4.0998, -4.0998, -4.0998, -4.0998, -4.0998, -4.0998, -4.0998, -4.1927, -4.8372, -4.9535, -4.9178, -4.96, -4.9246, -4.9664, -5.1085, -5.1085, -4.0998, -4.0998, -5.2046, -4.7938, -4.9531], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.1248, 1.1225, 1.0694, 1.043, 1.0362, 1.0142, 0.9994, 0.9876, 0.9813, 0.9734, 0.9721, 0.9692, 0.9384, 0.9353, 0.9341, 0.9286, 0.9286, 0.9286, 0.9286, 0.9286, 0.9286, 0.9286, 0.9286, 0.9286, 0.9286, 0.9286, 0.9286, 0.9286, 0.9286, 0.9286, 0.9286, 0.775, 0.5501, 0.6815, 0.6537, -0.346, 1.2146, 1.213, 1.2066, 1.1856, 1.1645, 1.1486, 1.1303, 1.085, 1.0729, 1.0556, 1.0015, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9697, 0.9697, 0.8196, 0.7597, 0.1821, 1.2683, 1.2463, 1.2204, 1.171, 1.1543, 1.1413, 1.14, 1.14, 1.1237, 1.1237, 1.1222, 1.1222, 1.0976, 1.0976, 1.0976, 1.0976, 1.0976, 1.0976, 1.0976, 1.0976, 1.0688, 1.0688, 1.0688, 1.0456, 1.0451, 1.0175, 1.0175, 1.0175, 1.0175, 1.0151, 0.9791, 0.9569, 0.6408, 0.725, 0.3302, 1.4063, 1.3689, 1.3001, 1.2973, 1.2921, 1.2822, 1.2638, 1.2638, 1.2638, 1.2638, 1.2638, 1.2638, 1.2638, 1.2638, 1.2638, 1.2638, 1.2638, 1.2638, 1.2172, 1.0732, 1.0399, 1.0333, 1.0319, 1.0199, 0.9995, 0.9781, 0.9781, 0.9535, 0.9535, 0.9276, 0.8062, 0.8322]}, \"token.table\": {\"Topic\": [2, 3, 2, 4, 3, 2, 4, 2, 4, 1, 2, 2, 3, 2, 3, 2, 1, 1, 1, 3, 1, 3, 1, 2, 1, 4, 3, 1, 3, 1, 2, 2, 1, 3, 1, 1, 2, 2, 3, 3, 4, 2, 4, 1, 3, 3, 1, 1, 3, 1, 3, 2, 2, 3, 2, 1, 3, 1, 4, 3, 1, 2, 3, 4, 4, 3, 2, 4, 4, 1, 4, 3, 4, 1, 4, 4, 2, 4, 2, 1, 3, 3, 3, 1, 3, 2, 3, 1, 1, 2, 4, 4, 4, 4, 2, 1, 3, 2, 4, 3, 1, 3, 2, 1, 4, 2, 1, 2, 1, 4, 4, 2, 2, 3, 1, 2, 3, 4, 1, 3, 4, 1, 3, 2, 4, 2, 4, 1, 2, 3, 4, 4, 1, 4, 3, 3, 1, 2, 3, 1, 1, 1, 4, 1], \"Freq\": [0.7102554746115021, 0.7399263006993106, 0.7257472123720549, 0.9902846640451672, 0.8234444049698789, 0.7514987508232382, 0.7907136272547496, 0.7102554746115021, 0.7907133379019244, 0.15935026204224711, 0.7967513102112356, 0.9655628946733502, 0.8234294098084736, 0.4426143073054914, 0.4426143073054914, 0.7514987508232382, 0.6620498510840128, 0.6620496336154734, 0.6620496336154734, 0.698825571239506, 0.6620496336154734, 0.7597366042019091, 0.6620496336154734, 0.7102554746115021, 0.6620498510840128, 0.7411184870210412, 0.8234294098084736, 0.468577625445533, 0.468577625445533, 0.35385735573102534, 0.7077147114620507, 0.7592381153623354, 0.6620498510840128, 0.6271551931156817, 0.6453785881326228, 0.6620496336154734, 0.9091431439493799, 0.8667705054330199, 0.7399263006993106, 0.6013806475902458, 0.9921160641390444, 0.7102554746115021, 0.7907133379019244, 0.5401034828411999, 0.6584527231111541, 0.6552467118583784, 0.8353846470650794, 0.3749909063244048, 0.7499818126488096, 0.49597839112416503, 0.49597839112416503, 0.7102555393062667, 0.6125288063754522, 0.7399263006993106, 0.7102554746115021, 0.5931793558759165, 0.8234294098084736, 0.6620496336154734, 0.7907133379019244, 0.6552467118583784, 0.9244903127038575, 0.7102555393062667, 0.698825571239506, 0.7907136272547496, 0.7907136272547496, 0.7802317252454187, 0.7102555393062667, 0.7907133379019244, 0.7067524289903855, 0.5618530077525488, 0.6830759217663885, 0.698825571239506, 0.9814060254271849, 0.2898842464288652, 0.5797684928577304, 0.8146579166113709, 0.5851093134896891, 0.8526810075911544, 1.0170291578075576, 0.41785402623429635, 0.41785402623429635, 0.7801201200228683, 0.6584527231111541, 0.6915955220696322, 0.2305318406898774, 0.7102555393062667, 0.698825571239506, 0.5846701733688283, 0.6620496336154734, 0.562549951319552, 0.7907133379019244, 0.8281959953313732, 0.9460452964059577, 0.7907133379019244, 0.7102554746115021, 0.8227440360680938, 0.698825571239506, 0.7102555393062667, 0.7220447618319581, 0.6271551931156817, 0.7728393177184135, 0.8748281404265744, 0.7102554746115021, 0.6620496336154734, 0.7907133379019244, 0.6386834146991217, 0.645189818418266, 0.8985191574101399, 0.6620496336154734, 0.9980220385859351, 0.7114267395608457, 0.7102554746115021, 0.7102554746115021, 0.973048316298592, 0.939582247019159, 0.7102555393062667, 0.698825571239506, 0.6027560537074926, 0.5670882204142252, 0.7214116026038164, 0.7907133379019244, 0.6620496336154734, 0.8234294098084736, 0.5008072347867694, 0.5008072347867694, 0.7102554746115021, 0.7907133379019244, 0.7979776428378175, 0.31470668382477746, 0.6294133676495549, 0.7421342002306448, 0.8146579166113709, 0.6620496336154734, 0.7231531331537652, 0.698825571239506, 0.5739412033228555, 0.6620496336154734, 0.7102555393062667, 0.698825571239506, 0.6454957792043061, 0.6064187847135225, 0.2898769863105914, 0.5797539726211828, 0.5848286496733223], \"Term\": [\"absence\", \"aggravated\", \"author\", \"book\", \"build\", \"cait\", \"cast\", \"cell\", \"certain\", \"character\", \"character\", \"cheesier\", \"cliffhanger\", \"conclusion\", \"conclusion\", \"confusing\", \"connected\", \"dealt\", \"detail\", \"development\", \"different\", \"disappointed\", \"done\", \"dropping\", \"echo\", \"end\", \"ending\", \"enough\", \"enough\", \"even\", \"even\", \"every\", \"experience\", \"explaining\", \"explanation\", \"fascination\", \"feel\", \"felt\", \"finally\", \"finished\", \"first\", \"fix\", \"flaw\", \"found\", \"fresh\", \"fully\", \"get\", \"good\", \"good\", \"got\", \"got\", \"grasp\", \"guy\", \"happy\", \"highlight\", \"honest\", \"honestly\", \"husband\", \"ignored\", \"information\", \"interesting\", \"introduced\", \"island\", \"justice\", \"killer\", \"kind\", \"language\", \"largely\", \"last\", \"lee\", \"left\", \"lexie\", \"like\", \"little\", \"little\", \"lot\", \"love\", \"low\", \"made\", \"make\", \"make\", \"mild\", \"mind\", \"much\", \"much\", \"new\", \"oaksey\", \"obsession\", \"ohlsson\", \"one\", \"overlooked\", \"page\", \"part\", \"pevel\", \"phone\", \"place\", \"possibly\", \"question\", \"read\", \"reader\", \"reading\", \"really\", \"reference\", \"relationship\", \"revelation\", \"right\", \"said\", \"scene\", \"seems\", \"sequel\", \"short\", \"simply\", \"someone\", \"something\", \"sort\", \"speaking\", \"spent\", \"star\", \"started\", \"story\", \"stronger\", \"study\", \"supposedly\", \"sure\", \"sure\", \"technology\", \"thanks\", \"though\", \"time\", \"time\", \"track\", \"unexplained\", \"unfaithful\", \"unfortunately\", \"villager\", \"wanted\", \"way\", \"whether\", \"wife\", \"world\", \"would\", \"writing\", \"writing\", \"xd\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [3, 1, 4, 2]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el980601402014234097765208497423\", ldavis_el980601402014234097765208497423_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el980601402014234097765208497423\", ldavis_el980601402014234097765208497423_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el980601402014234097765208497423\", ldavis_el980601402014234097765208497423_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "2     -0.069898  0.099067       1        1  28.187286\n",
       "0     -0.065162 -0.123024       2        1  25.500343\n",
       "3     -0.040342  0.024539       3        1  23.306665\n",
       "1      0.175403 -0.000581       4        1  23.005706, topic_info=          Term      Freq     Total Category  logprob  loglift\n",
       "11        book  8.000000  8.000000  Default  30.0000  30.0000\n",
       "15       first  5.000000  5.000000  Default  29.0000  29.0000\n",
       "115  character  6.000000  6.000000  Default  28.0000  28.0000\n",
       "308       part  3.000000  3.000000  Default  27.0000  27.0000\n",
       "131       felt  3.000000  3.000000  Default  26.0000  26.0000\n",
       "..         ...       ...       ...      ...      ...      ...\n",
       "364     little  2.059252  3.449653   Topic4  -4.0998   0.9535\n",
       "362    writing  2.059253  3.449739   Topic4  -4.0998   0.9535\n",
       "91         low  0.682215  1.172772   Topic4  -5.2046   0.9276\n",
       "174       sure  1.028746  1.996776   Topic4  -4.7938   0.8062\n",
       "34        star  0.877262  1.659046   Topic4  -4.9531   0.8322\n",
       "\n",
       "[166 rows x 6 columns], token_table=      Topic      Freq        Term\n",
       "term                             \n",
       "309       2  0.710255     absence\n",
       "302       3  0.739926  aggravated\n",
       "35        2  0.725747      author\n",
       "11        4  0.990285        book\n",
       "251       3  0.823444       build\n",
       "...     ...       ...         ...\n",
       "85        1  0.645496       world\n",
       "65        1  0.606419       would\n",
       "362       1  0.289877     writing\n",
       "362       4  0.579754     writing\n",
       "203       1  0.584829          xd\n",
       "\n",
       "[144 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[3, 1, 4, 2])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis_neg = pyLDAvis.gensim_models.prepare(lda_model_neg_100p, corpus_neg, id2word)\n",
    "vis_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cf9cab78",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(vis_neg, 'lda_visualization.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
